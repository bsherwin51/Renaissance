{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "814ea885-62e3-4cba-b16f-b67d2f72ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c618b9e6-ae4d-41b1-97a0-6e9330244f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"catalog.h5\" (mode r)>\n"
     ]
    }
   ],
   "source": [
    "ds = h.File('catalog.h5')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d152c7-8e20-4562-b5be-f17cd1aa7933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['Halo00000000', 'Halo00000001', 'Halo00000002', 'Halo00000003', 'Halo00000004', 'Halo00000005', 'Halo00000006', 'Halo00000007', 'Halo00000008', 'Halo00000009', 'Halo00000010', 'Halo00000011', 'Halo00000012', 'Halo00000013', 'Halo00000014', 'Halo00000015', 'Halo00000016', 'Halo00000017', 'Halo00000018', 'Halo00000019', 'Halo00000020', 'Halo00000021', 'Halo00000022', 'Halo00000023', 'Halo00000024', 'Halo00000025', 'Halo00000026', 'Halo00000027', 'Halo00000028', 'Halo00000029', 'Halo00000030', 'Halo00000031', 'Halo00000032', 'Halo00000033', 'Halo00000034', 'Halo00000035', 'Halo00000036', 'Halo00000037', 'Halo00000038', 'Halo00000039', 'Halo00000040', 'Halo00000041', 'Halo00000042', 'Halo00000043', 'Halo00000044', 'Halo00000045', 'Halo00000046', 'Halo00000047', 'Halo00000048', 'Halo00000049', 'Halo00000050', 'Halo00000051', 'Halo00000052', 'Halo00000053', 'Halo00000054', 'Halo00000055', 'Halo00000056', 'Halo00000057', 'Halo00000058', 'Halo00000059', 'Halo00000060', 'Halo00000061', 'Halo00000062', 'Halo00000063', 'Halo00000064', 'Halo00000065', 'Halo00000066', 'Halo00000067', 'Halo00000068', 'Halo00000069', 'Halo00000070', 'Halo00000071', 'Halo00000072', 'Halo00000073', 'Halo00000074', 'Halo00000075', 'Halo00000076', 'Halo00000077', 'Halo00000078', 'Halo00000079', 'Halo00000080', 'Halo00000081', 'Halo00000082', 'Halo00000083', 'Halo00000084', 'Halo00000085', 'Halo00000086', 'Halo00000087', 'Halo00000088', 'Halo00000089', 'Halo00000090', 'Halo00000091', 'Halo00000092', 'Halo00000093', 'Halo00000094', 'Halo00000095', 'Halo00000096', 'Halo00000097', 'Halo00000098', 'Halo00000099', 'Halo00000100', 'Halo00000101', 'Halo00000102', 'Halo00000103', 'Halo00000104', 'Halo00000105', 'Halo00000106', 'Halo00000107', 'Halo00000108', 'Halo00000109', 'Halo00000110', 'Halo00000111', 'Halo00000112', 'Halo00000113', 'Halo00000114', 'Halo00000115', 'Halo00000116', 'Halo00000117', 'Halo00000118', 'Halo00000119', 'Halo00000120', 'Halo00000121', 'Halo00000122', 'Halo00000123', 'Halo00000124', 'Halo00000125', 'Halo00000126', 'Halo00000127', 'Halo00000128', 'Halo00000129', 'Halo00000130', 'Halo00000131', 'Halo00000132', 'Halo00000133', 'Halo00000134', 'Halo00000135', 'Halo00000136', 'Halo00000137', 'Halo00000138', 'Halo00000139', 'Halo00000140', 'Halo00000141', 'Halo00000142', 'Halo00000143', 'Halo00000144', 'Halo00000145', 'Halo00000146', 'Halo00000147', 'Halo00000148', 'Halo00000149', 'Halo00000150', 'Halo00000151', 'Halo00000152', 'Halo00000153', 'Halo00000154', 'Halo00000155', 'Halo00000156', 'Halo00000157', 'Halo00000158', 'Halo00000159', 'Halo00000160', 'Halo00000161', 'Halo00000162', 'Halo00000163', 'Halo00000164', 'Halo00000165', 'Halo00000166', 'Halo00000167', 'Halo00000168', 'Halo00000169', 'Halo00000170', 'Halo00000171', 'Halo00000172', 'Halo00000173', 'Halo00000174', 'Halo00000175', 'Halo00000176', 'Halo00000177', 'Halo00000178', 'Halo00000179', 'Halo00000180', 'Halo00000181', 'Halo00000182', 'Halo00000183', 'Halo00000184', 'Halo00000185', 'Halo00000186', 'Halo00000187', 'Halo00000188', 'Halo00000189', 'Halo00000190', 'Halo00000191', 'Halo00000192', 'Halo00000193', 'Halo00000194', 'Halo00000195', 'Halo00000196', 'Halo00000197', 'Halo00000198', 'Halo00000199', 'Halo00000200', 'Halo00000201', 'Halo00000202', 'Halo00000203', 'Halo00000204', 'Halo00000205', 'Halo00000206', 'Halo00000207', 'Halo00000208', 'Halo00000209', 'Halo00000210', 'Halo00000211', 'Halo00000212', 'Halo00000213', 'Halo00000214', 'Halo00000215', 'Halo00000216', 'Halo00000217', 'Halo00000218', 'Halo00000219', 'Halo00000220', 'Halo00000221', 'Halo00000222', 'Halo00000223', 'Halo00000224', 'Halo00000225', 'Halo00000226', 'Halo00000227', 'Halo00000228', 'Halo00000229', 'Halo00000230', 'Halo00000231', 'Halo00000232', 'Halo00000233', 'Halo00000234', 'Halo00000235', 'Halo00000236', 'Halo00000237', 'Halo00000238', 'Halo00000239', 'Halo00000240', 'Halo00000241', 'Halo00000242', 'Halo00000243', 'Halo00000244', 'Halo00000245', 'Halo00000246', 'Halo00000247', 'Halo00000248', 'Halo00000249', 'Halo00000250', 'Halo00000251', 'Halo00000252', 'Halo00000253', 'Halo00000254', 'Halo00000255', 'Halo00000256', 'Halo00000257', 'Halo00000258', 'Halo00000259', 'Halo00000260', 'Halo00000261', 'Halo00000262', 'Halo00000263', 'Halo00000264', 'Halo00000265', 'Halo00000266', 'Halo00000267', 'Halo00000268', 'Halo00000269', 'Halo00000270', 'Halo00000271', 'Halo00000272', 'Halo00000273', 'Halo00000274', 'Halo00000275', 'Halo00000276', 'Halo00000277', 'Halo00000278', 'Halo00000279', 'Halo00000280', 'Halo00000281', 'Halo00000282', 'Halo00000283', 'Halo00000284', 'Halo00000285', 'Halo00000286', 'Halo00000287', 'Halo00000288', 'Halo00000289', 'Halo00000290', 'Halo00000291', 'Halo00000292', 'Halo00000293', 'Halo00000294', 'Halo00000295']>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b635dbf2-b79d-4bea-8390-dda1c509ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['M_HI', 'M_HII', 'Mstar', 'Mstar_pop2', 'Mstar_pop2_young', 'Mstar_pop3', 'Nesc', 'Ntot', 'NumberOfPop2Stars', 'NumberOfPop3Stars', 'SFR', 'center', 'fesc', 'fgas', 'fstar', 'mass', 'redshift', 'rvir']>\n"
     ]
    }
   ],
   "source": [
    "for key in ds.keys():\n",
    "    print(ds[key].keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0b0f24-d8d1-4bb5-87d5-276b9ef6687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to hdf5 file\n",
    "import yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc7186c-4660-48fa-abed-b0888811e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "half_rad = []\n",
    "for halo in ds.keys():\n",
    "    center = list(ds[halo]['SFR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95f3e60-5af8-4283-a668-785f919df92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.510385777184007e-05,\n",
       " 5.036834833717145e-05,\n",
       " 0.001991233705686818,\n",
       " 0.004023154669108055,\n",
       " 0.005117357639142305,\n",
       " 0.0058649787944295905,\n",
       " 0.006344632829132147,\n",
       " 0.001784146027742063,\n",
       " 0.0008458373783667337,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533461e0-90f8-458f-ac99-0942f233a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a389129-b5e0-4521-91e0-b8f86485796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3122 3122 3122 3122 3122 3122\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "all_groups = {}\n",
    "fescs = []\n",
    "groups = []\n",
    "fgases = []\n",
    "masses = []\n",
    "SFRs = [] \n",
    "centers = []\n",
    "Mstars = []\n",
    "redshifts = []\n",
    "for halo in ds.keys(): # loops through halos\n",
    "    fesc_list = list(ds[halo]['fesc'])\n",
    "    fgas_list = list(ds[halo]['fgas'])\n",
    "    mass_list = list(ds[halo]['mass'])\n",
    "    SFR_list = list(ds[halo]['SFR'])\n",
    "    Mstar_list = list(ds[halo]['Mstar'])\n",
    "    redshift_list = list(ds[halo]['redshift'])    \n",
    "    for index, fesc in enumerate(fesc_list):# loops thru timestep\n",
    "        if fesc <= 0.01:\n",
    "            group = 0\n",
    "        elif 0.01 < fesc <= 0.10:\n",
    "            group = 1\n",
    "        elif 0.10 < fesc <= 0.25:\n",
    "            group = 2\n",
    "        else:\n",
    "            group = 3\n",
    "        # used enumerate to find index and only use \"good\" values\n",
    "        if fesc >= 1e-5:\n",
    "            fescs.append(fesc)\n",
    "            groups.append(group)\n",
    "            fgases.append(ds[halo]['fgas'][index])\n",
    "            masses.append(ds[halo]['mass'][index])\n",
    "            SFRs.append(ds[halo]['SFR'][index])\n",
    "            Mstars.append(ds[halo]['Mstar'][index])\n",
    "            redshifts.append(ds[halo]['redshift'][index])\n",
    "            centers.append(ds[halo]['center'][index])\n",
    "    all_groups[halo] = groups\n",
    "\n",
    "# Sanity check: All parameter arrays should be same size\n",
    "print(len(fescs), len(fgases), len(masses), len(SFRs), len(Mstars), len(redshifts)) # length is 3122 for all\n",
    "\n",
    "# Sanity check 2: Length of all_groups should be same as number of halos\n",
    "print(len(ds.keys()) == len(all_groups)) # length is 296\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f74fc91-3f49-4b9b-8f38-861614a2c796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.59921885 11.7000254  11.80081925 11.8998968  12.00052002 12.09929264\n",
      " 12.19957761 12.29964091 12.39943722 12.50074254 12.5998912  12.70050692\n",
      " 12.80071764 12.90047262 12.99972001 13.10039481 13.20051122 13.3000143\n",
      " 13.40092166 13.49905756 13.60067163 13.69939732 13.79946722 13.90090896\n",
      " 13.99925004 14.10117789 14.1998784  14.2998776  14.40120129 14.49907006\n",
      " 14.60062402 14.70105197 14.80028441 14.90077914 15.         15.10046691\n",
      " 15.19957881 15.2999185  15.39881929 15.49892757 15.6002656  15.7000668\n",
      " 15.80107527 15.90045631 16.00102006 16.1998624  16.40038281 16.59943682\n",
      " 16.7999288  16.99856012 17.20167455 17.39926403 17.60119048 17.80052641\n",
      " 18.00057002 18.20122888 18.3986421  18.6001568  18.79805979 19.\n",
      " 19.2020202  19.3998368  19.60156572 19.79866889 19.99958001 20.50075253\n",
      " 20.99736032 21.50225023 21.99908004 22.50176263 22.99808015 25.00104004]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(redshifts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "02ed016b-68ff-4a05-b58e-bc08b411eb53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69, 56, 66, 43, 43, 68, 75, 28, 65, 71, 65, 65, 67, 75, 75, 58, 51, 40, 55, 68, 68, 71, 75, 75, 65, 66, 71, 49, 66, 70, 56, 34, 67, 66, 59, 73, 68, 73, 49, 15, 69, 54, 66, 63, 71, 56, 54, 71, 40, 71, 57, 73, 23, 49, 69, 71, 73, 70, 69, 58, 67, 70, 48, 71, 54, 45, 67, 58, 68, 49, 71, 74, 21, 57, 66, 68, 72, 65, 75, 66, 32, 66, 63, 69, 49, 68, 72, 59, 67, 73, 63, 67, 48, 52, 65, 65, 36, 66, 26, 63, 72, 75, 70, 68, 74, 39, 64, 57, 72, 58, 33, 68, 25, 66, 56, 56, 25, 44, 71, 51, 72, 67, 30, 65, 51, 48, 64, 50, 73, 19, 65, 57, 57, 56, 63, 64, 36, 37, 71, 73, 66, 75, 35, 75, 25, 49, 51, 65, 56, 74, 68, 62, 60, 67, 58, 69, 59, 66, 33, 70, 38, 49, 67, 65, 75, 28, 66, 59, 29, 51, 70, 61, 65, 56, 49, 67, 62, 69, 70, 38, 28, 61, 7, 71, 17, 36, 20, 67, 75, 65, 69, 50, 58, 58, 25, 47, 66, 59, 40, 65, 61, 60, 68, 64, 49, 65, 27, 63, 63, 67, 48, 64, 66, 56, 62, 26, 32, 66, 22, 69, 20, 23, 74, 65, 55, 65, 60, 29, 67, 65, 72, 65, 58, 47, 51, 67, 32, 56, 38, 41, 34, 71, 68, 62, 25, 32, 75, 69, 66, 70, 25, 69, 21, 53, 71, 46, 18, 57, 74, 55, 66, 64, 69, 72, 30, 47, 49, 70, 19, 48, 65, 68, 66, 59, 56, 64, 70, 58, 41, 62, 41, 19, 63, 75, 68, 65, 62, 68, 45, 54, 69, 70, 56, 68, 72, 63]\n"
     ]
    }
   ],
   "source": [
    "# import numpy.ma as ma\n",
    "        \n",
    "# # 296 halos, 75 redshifts\n",
    "# all_redshift_lengths = []\n",
    "# for halo in ds.keys():\n",
    "#     redshift_length = len(list(ds[halo]['redshift']))\n",
    "#     all_redshift_lengths.append(redshift_length)\n",
    "     \n",
    "\n",
    "# all_centers = np.zeros((len(ds.keys()), max(all_redshift_lengths), 3))\n",
    "# all_fesc = np.zeros((len(ds.keys()), max(all_redshift_lengths)))\n",
    "# mask_centers = np.zeros((len(ds.keys()), max(all_redshift_lengths), 3), dtype = bool)\n",
    "# mask_fesc = np.zeros((len(ds.keys()), max(all_redshift_lengths)), dtype = bool)\n",
    "\n",
    "# for i, length in enumerate(all_redshift_lengths):\n",
    "#     for j in range(length):\n",
    "#         mask[i, j, :] = True, True, True\n",
    "        \n",
    "# for i, halo in enumerate(ds.keys()):\n",
    "#     for j, center in enumerate(list(ds[halo]['center'])):\n",
    "#         all_centers[i,j,:] = np.array(center)\n",
    "#     for j, fesc in enumerate(list(ds[halo]['fesc'])): \n",
    "#         all_fesc[i,j] = fesc\n",
    "\n",
    "# all_centers = ma.array(all_centers, mask = mask_centers)\n",
    "# all_fesc =  ma.array(all_fesc, mask = mask_fesc)\n",
    "        \n",
    "# distance =  np.zeros((len(ds.keys()), max(all_redshift_lengths)))\n",
    "# distance.fill(np.inf)\n",
    "# for halo_no in range(len(ds.keys())):\n",
    "#     for z in range(max(all_redshift_lengths)):\n",
    "#         center = all_centers[halo_no,z]\n",
    "#         distance[:,z] = np.linalg.norm(all_centers[:,z]-center, axis = 1)\n",
    "\n",
    "    \n",
    "# print(all_redshift_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5396be30-f7ad-4637-a94d-1ea9108ccab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (np.array(all_redshift_lengths) <= 69).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "93dfe287-8377-4693-be3f-2df46d1e6c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "i = 1\n",
      "i = 2\n",
      "i = 3\n",
      "i = 4\n",
      "i = 5\n",
      "i = 6\n",
      "i = 7\n",
      "i = 8\n",
      "i = 9\n",
      "i = 10\n",
      "i = 11\n",
      "i = 12\n",
      "i = 13\n",
      "i = 14\n",
      "i = 15\n",
      "i = 16\n",
      "i = 17\n",
      "i = 18\n",
      "i = 19\n",
      "i = 20\n",
      "i = 21\n",
      "i = 22\n",
      "i = 23\n",
      "i = 24\n",
      "i = 25\n",
      "i = 26\n",
      "i = 27\n",
      "i = 28\n",
      "i = 29\n",
      "i = 30\n",
      "i = 31\n",
      "i = 32\n",
      "i = 33\n",
      "i = 34\n",
      "i = 35\n",
      "i = 36\n",
      "i = 37\n",
      "i = 38\n",
      "i = 39\n",
      "i = 40\n",
      "i = 41\n",
      "i = 42\n",
      "i = 43\n",
      "i = 44\n",
      "i = 45\n",
      "i = 46\n",
      "i = 47\n",
      "i = 48\n",
      "i = 49\n",
      "i = 50\n",
      "i = 51\n",
      "i = 52\n",
      "i = 53\n",
      "i = 54\n",
      "i = 55\n",
      "i = 56\n",
      "i = 57\n",
      "i = 58\n",
      "i = 59\n",
      "i = 60\n",
      "i = 61\n",
      "i = 62\n",
      "i = 63\n",
      "i = 64\n",
      "i = 65\n",
      "i = 66\n",
      "i = 67\n",
      "i = 68\n",
      "i = 69\n",
      "i = 70\n",
      "i = 71\n",
      "i = 72\n",
      "i = 73\n",
      "i = 74\n",
      "i = 75\n",
      "i = 76\n",
      "i = 77\n",
      "i = 78\n",
      "i = 79\n",
      "i = 80\n",
      "i = 81\n",
      "i = 82\n",
      "i = 83\n",
      "i = 84\n",
      "i = 85\n",
      "i = 86\n",
      "i = 87\n",
      "i = 88\n",
      "i = 89\n",
      "i = 90\n",
      "i = 91\n",
      "i = 92\n",
      "i = 93\n",
      "i = 94\n",
      "i = 95\n",
      "i = 96\n",
      "i = 97\n",
      "i = 98\n",
      "i = 99\n",
      "i = 100\n",
      "i = 101\n",
      "i = 102\n",
      "i = 103\n",
      "i = 104\n",
      "i = 105\n",
      "i = 106\n",
      "i = 107\n",
      "i = 108\n",
      "i = 109\n",
      "i = 110\n",
      "i = 111\n",
      "i = 112\n",
      "i = 113\n",
      "i = 114\n",
      "i = 115\n",
      "i = 116\n",
      "i = 117\n",
      "i = 118\n",
      "i = 119\n",
      "i = 120\n",
      "i = 121\n",
      "i = 122\n",
      "i = 123\n",
      "i = 124\n",
      "i = 125\n",
      "i = 126\n",
      "i = 127\n",
      "i = 128\n",
      "i = 129\n",
      "i = 130\n",
      "i = 131\n",
      "i = 132\n",
      "i = 133\n",
      "i = 134\n",
      "i = 135\n",
      "i = 136\n",
      "i = 137\n",
      "i = 138\n",
      "i = 139\n",
      "i = 140\n",
      "i = 141\n",
      "i = 142\n",
      "i = 143\n",
      "i = 144\n",
      "i = 145\n",
      "i = 146\n",
      "i = 147\n",
      "i = 148\n",
      "i = 149\n",
      "i = 150\n",
      "i = 151\n",
      "i = 152\n",
      "i = 153\n",
      "i = 154\n",
      "i = 155\n",
      "i = 156\n",
      "i = 157\n",
      "i = 158\n",
      "i = 159\n",
      "i = 160\n",
      "i = 161\n",
      "i = 162\n",
      "i = 163\n",
      "i = 164\n",
      "i = 165\n",
      "i = 166\n",
      "i = 167\n",
      "i = 168\n",
      "i = 169\n",
      "i = 170\n",
      "i = 171\n",
      "i = 172\n",
      "i = 173\n",
      "i = 174\n",
      "i = 175\n",
      "i = 176\n",
      "i = 177\n",
      "i = 178\n",
      "i = 179\n",
      "i = 180\n",
      "i = 181\n",
      "i = 182\n",
      "i = 183\n",
      "i = 184\n",
      "i = 185\n",
      "i = 186\n",
      "i = 187\n",
      "i = 188\n",
      "i = 189\n",
      "i = 190\n",
      "i = 191\n",
      "i = 192\n",
      "i = 193\n",
      "i = 194\n",
      "i = 195\n",
      "i = 196\n",
      "i = 197\n",
      "i = 198\n",
      "i = 199\n",
      "i = 200\n",
      "i = 201\n",
      "i = 202\n",
      "i = 203\n",
      "i = 204\n",
      "i = 205\n",
      "i = 206\n",
      "i = 207\n",
      "i = 208\n",
      "i = 209\n",
      "i = 210\n",
      "i = 211\n",
      "i = 212\n",
      "i = 213\n",
      "i = 214\n",
      "i = 215\n",
      "i = 216\n",
      "i = 217\n",
      "i = 218\n",
      "i = 219\n",
      "i = 220\n",
      "i = 221\n",
      "i = 222\n",
      "i = 223\n",
      "i = 224\n",
      "i = 225\n",
      "i = 226\n",
      "i = 227\n",
      "i = 228\n",
      "i = 229\n",
      "i = 230\n",
      "i = 231\n",
      "i = 232\n",
      "i = 233\n",
      "i = 234\n",
      "i = 235\n",
      "i = 236\n",
      "i = 237\n",
      "i = 238\n",
      "i = 239\n",
      "i = 240\n",
      "i = 241\n",
      "i = 242\n",
      "i = 243\n",
      "i = 244\n",
      "i = 245\n",
      "i = 246\n",
      "i = 247\n",
      "i = 248\n",
      "i = 249\n",
      "i = 250\n",
      "i = 251\n",
      "i = 252\n",
      "i = 253\n",
      "i = 254\n",
      "i = 255\n",
      "i = 256\n",
      "i = 257\n",
      "i = 258\n",
      "i = 259\n",
      "i = 260\n",
      "i = 261\n",
      "i = 262\n",
      "i = 263\n",
      "i = 264\n",
      "i = 265\n",
      "i = 266\n",
      "i = 267\n",
      "i = 268\n",
      "i = 269\n",
      "i = 270\n",
      "i = 271\n",
      "i = 272\n",
      "i = 273\n",
      "i = 274\n",
      "i = 275\n",
      "i = 276\n",
      "i = 277\n",
      "i = 278\n",
      "i = 279\n",
      "i = 280\n",
      "i = 281\n",
      "i = 282\n",
      "i = 283\n",
      "i = 284\n",
      "i = 285\n",
      "i = 286\n",
      "i = 287\n",
      "i = 288\n",
      "i = 289\n",
      "i = 290\n",
      "i = 291\n",
      "i = 292\n",
      "i = 293\n",
      "i = 294\n",
      "i = 295\n"
     ]
    }
   ],
   "source": [
    "closest_halo_dist = [] # all halos concatenation of min_dist_at_z's\n",
    "\n",
    "for i, halo1 in enumerate(ds.keys()):\n",
    "    print(f\"i = {i}\")\n",
    "    center_list = list(ds[halo1]['center'])\n",
    "    n_redshifts_halo1 = len(list(ds[halo1]['redshift']))\n",
    "    min_dist_at_z = [] # array of minimum distances at each redshift for halo N\n",
    "    \n",
    "    for z in range(n_redshifts_halo1):\n",
    "        dist_to_halo1= [] # array of ALL distances at each redshift for halo N\n",
    "        \n",
    "        for j,halo2 in enumerate(ds.keys()):\n",
    "            \n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            center_list2 = list(ds[halo2]['center'])\n",
    "            n_redshifts_halo2 = len(list(ds[halo2]['redshift']))\n",
    "           \n",
    "            \n",
    "            if z >= n_redshifts_halo2:\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            halo1_center = center_list[z]\n",
    "            halo2_center = center_list2[z]\n",
    "            \n",
    "    \n",
    "            dist_to_halo1.append(np.sqrt((halo1_center[0]-halo2_center[0])**2 + (halo1_center[1]-halo2_center[1])**2 + (halo1_center[2]-halo2_center[2])**2 ))\n",
    "        \n",
    "\n",
    "        if len(dist_to_halo1) == 0:\n",
    "            continue\n",
    "            \n",
    "        min_dist_at_z.append(min(dist_to_halo1)) # PROBLEM WITH THIS\n",
    "    \n",
    "    closest_halo_dist.append(min_dist_at_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf6b4d-7d16-4f55-bb1b-37fa33dc6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(closest_halo_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3fe264bb-6212-4bd4-8e45-51dc1f83f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1498/1373678359.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(closest_halo_dist)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array(closest_halo_dist)\n",
    "np.save('min_dist_normal', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7193dc48-9e88-492a-849c-84b6c97bd854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "print(len(closest_halo_dist[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "514ad4a6-b39c-49c9-a007-e7e17426b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yt.utilities.cosmology import Cosmology\n",
    "\n",
    "co = Cosmology()\n",
    "\n",
    " \n",
    "all_t_lookbacks = []\n",
    "for halo in ds.keys():\n",
    "    t_lookbacks_halo = []\n",
    "    for z_idx in range(len(list(ds[halo]['redshift']))):\n",
    "        t_lookback = co.t_from_z(0) - co.t_from_z(ds[halo]['redshift'][z_idx])\n",
    "        t_lookbacks_halo.append(t_lookback)\n",
    "    all_t_lookbacks.append(t_lookbacks_halo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1482ffdf-6310-4156-97c1-bda6613d9d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1498/115754983.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr2 = np.array(all_t_lookbacks)\n"
     ]
    }
   ],
   "source": [
    "arr2 = np.array(all_t_lookbacks)\n",
    "np.save('t_lookback_normal', arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b1f8500-9328-4f86-865e-1c6e42196ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lookback = np.load('t_lookback_normal.npy', allow_pickle = True)\n",
    "min_dist = np.load('min_dist_normal.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbe6ef54-e257-4152-bb1a-e59d116607d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'NORM: {np.linalg.norm(all_centers[:,z]-center, axis = 1)}')\n",
    "# print(f'ALL CENTERS {all_centers[:,z]}')\n",
    "# print(f'CENTER:{center}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85b18d3c-5078-4214-9232-ede0663bba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001784146027742063\n"
     ]
    }
   ],
   "source": [
    "# np.min(SFRs[SFRs != 0])\n",
    "minsfr = 100\n",
    "for each in SFRs:\n",
    "    if each < minsfr and each != 0.:\n",
    "        minsfr = each\n",
    "print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa44db10-a6a5-4da1-bffb-4918b604b3c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3380224926.992715,\n",
       " 3299024674.3191757,\n",
       " 3215556588.5356255,\n",
       " 3073838923.8206606,\n",
       " 2913785711.4873447,\n",
       " 2570655484.3533587,\n",
       " 2505834729.167289,\n",
       " 2179203186.494319,\n",
       " 2211860439.447175,\n",
       " 2065288017.7761762,\n",
       " 1919810286.4714735,\n",
       " 1926627326.835315,\n",
       " 1865044999.2645564,\n",
       " 1783539688.5521364,\n",
       " 1735776109.215406,\n",
       " 1637180689.951307,\n",
       " 1544123131.7256577,\n",
       " 1509318050.1285434,\n",
       " 1459479703.8098915,\n",
       " 1248771742.1926913,\n",
       " 1355361377.037151,\n",
       " 1283150317.9564598,\n",
       " 1181455323.4343517,\n",
       " 1073240290.0893987,\n",
       " 976530180.0497017,\n",
       " 913880651.8249165,\n",
       " 870989705.1505412,\n",
       " 844812887.7485251,\n",
       " 824996976.5826399,\n",
       " 803019299.6994716,\n",
       " 777856081.5802172,\n",
       " 750390965.2046118,\n",
       " 726505066.4823706,\n",
       " 694341775.3481793,\n",
       " 652087701.6425964,\n",
       " 601432744.4017963,\n",
       " 520968863.5503441,\n",
       " 503570456.4282901,\n",
       " 466475690.4399628,\n",
       " 446138054.0223443,\n",
       " 418339093.33234394,\n",
       " 391071675.34323955,\n",
       " 342665675.71371824,\n",
       " 325958161.47398007,\n",
       " 314420966.1949795,\n",
       " 290128810.875418,\n",
       " 266585862.57688087,\n",
       " 244384647.05748343,\n",
       " 223666328.5336163,\n",
       " 201670167.38130778,\n",
       " 188303395.58719042,\n",
       " 171340980.45853704,\n",
       " 156436520.9307517,\n",
       " 139611835.31927726,\n",
       " 122625014.126394,\n",
       " 116407392.68516354,\n",
       " 109570630.73319837,\n",
       " 103476134.35442151,\n",
       " 97638470.75780873,\n",
       " 91566848.16727524,\n",
       " 84996536.10170776,\n",
       " 79006721.71217097,\n",
       " 75919349.78446332,\n",
       " 58185713.48136817,\n",
       " 47752596.84699674,\n",
       " 35492031.25324304,\n",
       " 1956132966.9928029,\n",
       " 1928916578.2740815,\n",
       " 720390077.7237056,\n",
       " 1826245572.4108236,\n",
       " 1309288888.3476744,\n",
       " 1434309255.9539683,\n",
       " 1807467429.330579,\n",
       " 1776273764.2357686,\n",
       " 1695056514.7792788,\n",
       " 1709758078.4092817,\n",
       " 1678394995.7944179,\n",
       " 1502860254.7398875,\n",
       " 1590524722.6681566,\n",
       " 1554351144.082302,\n",
       " 1487971906.3338413,\n",
       " 1518852224.2406018,\n",
       " 1489394905.8839157,\n",
       " 1397933038.2689488,\n",
       " 1298339568.3513331,\n",
       " 1213595371.1490705,\n",
       " 1165925000.945489,\n",
       " 1131840628.8230095,\n",
       " 1131507866.5389047,\n",
       " 1102035962.0609248,\n",
       " 1059560192.9473251,\n",
       " 1019155097.699731,\n",
       " 965395195.3023318,\n",
       " 660730020.591383,\n",
       " 703181547.5367141,\n",
       " 630531999.5834448,\n",
       " 616635548.430348,\n",
       " 601323093.7715743,\n",
       " 585170936.9341366,\n",
       " 571748156.6736176,\n",
       " 542163924.3763653,\n",
       " 537466902.1881827,\n",
       " 522499213.4386241,\n",
       " 492790213.65052533,\n",
       " 471358559.3373924,\n",
       " 461192129.27624404,\n",
       " 432487108.7134597,\n",
       " 388223072.7302692,\n",
       " 344354965.4041068,\n",
       " 292871618.4112299,\n",
       " 202312733.84257424,\n",
       " 174090112.65888345,\n",
       " 149313482.9733866,\n",
       " 125952814.21892352,\n",
       " 1324792223.370488,\n",
       " 1208396624.476195,\n",
       " 1143760453.26487,\n",
       " 1076083195.3380752,\n",
       " 1020076531.1549383,\n",
       " 979599233.9277216,\n",
       " 931002685.7703651,\n",
       " 887139628.6030937,\n",
       " 827754620.8081433,\n",
       " 732545424.028069,\n",
       " 684865176.923013,\n",
       " 655814801.3087095,\n",
       " 633577659.6836098,\n",
       " 603078861.7870522,\n",
       " 585141619.7083571,\n",
       " 562834220.4097619,\n",
       " 540974037.5851645,\n",
       " 519494482.58518565,\n",
       " 492243991.7916561,\n",
       " 470980498.89993584,\n",
       " 452474524.11691207,\n",
       " 432885359.5062258,\n",
       " 413024218.2561826,\n",
       " 392793665.0888341,\n",
       " 375065336.7379287,\n",
       " 343521766.87533367,\n",
       " 249800619.9812909,\n",
       " 238355078.5595831,\n",
       " 228996696.3475105,\n",
       " 220604911.90873674,\n",
       " 213043148.06611875,\n",
       " 204296656.03696582,\n",
       " 196668888.7618157,\n",
       " 59130366.79240656,\n",
       " 40716155.53767626,\n",
       " 19575535.25546923,\n",
       " 18458056.786956392,\n",
       " 15303658.091860294,\n",
       " 12008793.617353117,\n",
       " 1108801766.6565547,\n",
       " 1194208203.1095457,\n",
       " 1211062788.9233167,\n",
       " 1170607189.5396283,\n",
       " 1140883458.99216,\n",
       " 1119096287.6455033,\n",
       " 1070398895.835043,\n",
       " 1076298483.1744664,\n",
       " 1044744573.2345339,\n",
       " 1005142201.7632283,\n",
       " 962151192.4590734,\n",
       " 918193400.0722435,\n",
       " 869428100.3496945,\n",
       " 811705834.1645414,\n",
       " 771650560.4487222,\n",
       " 720151527.9498749,\n",
       " 685829106.668773,\n",
       " 658650243.3854967,\n",
       " 636985819.0348386,\n",
       " 615431940.8756008,\n",
       " 594080240.213202,\n",
       " 572483188.4487661,\n",
       " 554447245.0782456,\n",
       " 525806082.4695715,\n",
       " 495432441.4640622,\n",
       " 480752108.27858573,\n",
       " 458085216.3334525,\n",
       " 431246091.4789267,\n",
       " 390144306.04128134,\n",
       " 335755693.36042476,\n",
       " 272336331.62669694,\n",
       " 240362296.15036038,\n",
       " 173679176.69148904,\n",
       " 158270184.69050077,\n",
       " 150417668.10196796,\n",
       " 139989095.83334962,\n",
       " 130088624.16258419,\n",
       " 5928398.132322622,\n",
       " 1135384973.463822,\n",
       " 1100145153.482549,\n",
       " 1063164831.7105145,\n",
       " 1024414079.361965,\n",
       " 988765099.2430878,\n",
       " 955910580.621741,\n",
       " 923099651.6709253,\n",
       " 864603349.0632577,\n",
       " 873302186.4939605,\n",
       " 835017957.9345329,\n",
       " 767816876.2503281,\n",
       " 734766818.3911626,\n",
       " 698522005.3721275,\n",
       " 653436764.6228411,\n",
       " 968000734.5376085,\n",
       " 928934544.2791,\n",
       " 929244831.3557341,\n",
       " 870026445.8171753,\n",
       " 832933042.6649649,\n",
       " 801953256.2800891,\n",
       " 774131163.3053459,\n",
       " 750048780.7976582,\n",
       " 728694575.1661818,\n",
       " 708839553.610034,\n",
       " 690818460.3982859,\n",
       " 644646692.4631039,\n",
       " 653188570.47508,\n",
       " 628440018.1510314,\n",
       " 597366513.8802292,\n",
       " 572735913.490672,\n",
       " 547167396.266996,\n",
       " 532092374.2085536,\n",
       " 516872644.06710476,\n",
       " 499906556.35045165,\n",
       " 479227863.5866721,\n",
       " 460318211.8173964,\n",
       " 445036323.25134945,\n",
       " 432374370.8218593,\n",
       " 391967630.90297717,\n",
       " 390396686.980882,\n",
       " 325377893.6587829,\n",
       " 294368063.8698773,\n",
       " 321668752.6403921,\n",
       " 304246601.47419,\n",
       " 216986164.24701363,\n",
       " 182769132.00444028,\n",
       " 163845091.49524796,\n",
       " 144768214.45957378,\n",
       " 116200981.71718341,\n",
       " 97538040.89399503,\n",
       " 90039205.34888184,\n",
       " 83413211.5492732,\n",
       " 78876957.38490464,\n",
       " 74557603.66760588,\n",
       " 67985295.48805209,\n",
       " 60489903.04491934,\n",
       " 52000631.346820936,\n",
       " 39558237.383027524,\n",
       " 33645715.83925226,\n",
       " 31585715.634934753,\n",
       " 28118389.689643566,\n",
       " 25869294.991991106,\n",
       " 1078922804.5833077,\n",
       " 1025931694.4454825,\n",
       " 892502783.4615086,\n",
       " 857638200.6401223,\n",
       " 875685283.2819248,\n",
       " 887682616.8862516,\n",
       " 874766470.9376575,\n",
       " 850525627.6134441,\n",
       " 796857888.6688131,\n",
       " 777701466.8827243,\n",
       " 732169763.6634922,\n",
       " 701572947.2090541,\n",
       " 633597478.7964609,\n",
       " 58707530.611371554,\n",
       " 51977226.27195001,\n",
       " 47365784.42355762,\n",
       " 35631167.05812084,\n",
       " 25946367.03378247,\n",
       " 24276887.05170206,\n",
       " 22763280.4564374,\n",
       " 18180667.96933664,\n",
       " 10523130.081824673,\n",
       " 8127934.0472255815,\n",
       " 5935576.65657513,\n",
       " 924302077.2211623,\n",
       " 881370040.2878869,\n",
       " 852702055.8999903,\n",
       " 822955988.6471066,\n",
       " 763166307.6908425,\n",
       " 618820741.7125852,\n",
       " 672785805.0677167,\n",
       " 629451296.7198592,\n",
       " 624298209.4157655,\n",
       " 582255437.6772401,\n",
       " 550320618.5089941,\n",
       " 512752028.7380337,\n",
       " 503634044.106334,\n",
       " 483346756.98711187,\n",
       " 425617201.87076366,\n",
       " 438805003.6995517,\n",
       " 345173593.2987255,\n",
       " 296591320.73865205,\n",
       " 213597685.6695423,\n",
       " 196206590.76834813,\n",
       " 149121341.84347937,\n",
       " 170744823.1078593,\n",
       " 194768383.17916754,\n",
       " 178570079.16122985,\n",
       " 156964945.73139274,\n",
       " 830972103.6210337,\n",
       " 597567969.3856009,\n",
       " 710123821.9496001,\n",
       " 729242558.5429575,\n",
       " 625450986.7787702,\n",
       " 476669230.4411931,\n",
       " 325428332.9879197,\n",
       " 91310966.97200386,\n",
       " 262856139.34141275,\n",
       " 427028542.0729573,\n",
       " 382951223.8514675,\n",
       " 396786715.6500102,\n",
       " 180410076.27347204,\n",
       " 328798241.9452736,\n",
       " 346059117.67821497,\n",
       " 238471445.47723696,\n",
       " 180474197.74284145,\n",
       " 133512883.59844042,\n",
       " 106585753.81353803,\n",
       " 52814067.09227138,\n",
       " 98308396.5620548,\n",
       " 89797036.12244865,\n",
       " 315363079.60063446,\n",
       " 347384226.614673,\n",
       " 217868614.48125476,\n",
       " 241129534.27836794,\n",
       " 273812197.58464694,\n",
       " 232550149.0987787,\n",
       " 351888450.2165681,\n",
       " 329105229.92844975,\n",
       " 341343062.8995408,\n",
       " 347904257.3443816,\n",
       " 340218688.06980866,\n",
       " 333261311.5934913,\n",
       " 319627317.91749305,\n",
       " 313850702.13078463,\n",
       " 316097629.3264063,\n",
       " 318647669.0610069,\n",
       " 321530823.41384196,\n",
       " 247530447.73481634,\n",
       " 229613492.83186942,\n",
       " 201636453.37380385,\n",
       " 133719954.46106975,\n",
       " 52115572.13129706,\n",
       " 146720735.347406,\n",
       " 23336466.54536554,\n",
       " 23009482.51850877,\n",
       " 20082228.24193005,\n",
       " 19808420.948245533,\n",
       " 20088042.104169663,\n",
       " 18816653.73594061,\n",
       " 855731214.836989,\n",
       " 821995493.1477026,\n",
       " 791767469.6438912,\n",
       " 768848455.7266624,\n",
       " 742050037.1133264,\n",
       " 696012125.2895231,\n",
       " 635561167.7332006,\n",
       " 582497018.618529,\n",
       " 541383368.6384352,\n",
       " 499201588.34553677,\n",
       " 458115460.94923246,\n",
       " 414000461.52047783,\n",
       " 374515092.11759776,\n",
       " 348233140.8395774,\n",
       " 325744615.83523506,\n",
       " 307623029.31997657,\n",
       " 289856863.452993,\n",
       " 272799851.2625909,\n",
       " 250890373.74583,\n",
       " 230658360.44665807,\n",
       " 216518265.14829054,\n",
       " 209763656.43681037,\n",
       " 203790653.46012244,\n",
       " 200909093.45166278,\n",
       " 199611360.21993643,\n",
       " 197389781.86083126,\n",
       " 196689391.57794073,\n",
       " 196760108.36519647,\n",
       " 195165262.86732328,\n",
       " 193035856.81404203,\n",
       " 189990547.66043302,\n",
       " 185745215.362272,\n",
       " 179358220.7068254,\n",
       " 170666670.39596075,\n",
       " 160405227.85899684,\n",
       " 151429458.72714272,\n",
       " 142780338.93860984,\n",
       " 136331598.30525798,\n",
       " 128570022.01073527,\n",
       " 122142604.93505025,\n",
       " 115741681.2453014,\n",
       " 111501760.4549687,\n",
       " 105005155.60683207,\n",
       " 100233724.95946628,\n",
       " 94199045.69032633,\n",
       " 82688746.33601646,\n",
       " 759332105.7755286,\n",
       " 727565642.3479909,\n",
       " 697254906.5875337,\n",
       " 663617170.5754735,\n",
       " 635430556.7830931,\n",
       " 606480950.5227004,\n",
       " 573267353.1250601,\n",
       " 526531306.923474,\n",
       " 269566666.46686226,\n",
       " 287451043.17403954,\n",
       " 183444703.4600243,\n",
       " 175449040.95017532,\n",
       " 171993194.47213745,\n",
       " 173798970.45660585,\n",
       " 165740328.5783068,\n",
       " 157843616.69439217,\n",
       " 148463253.39415532,\n",
       " 127977656.31427637,\n",
       " 120570467.18261918,\n",
       " 119825758.07272433,\n",
       " 115567478.28283042,\n",
       " 111329584.26128599,\n",
       " 107164455.44456108,\n",
       " 104485929.394563,\n",
       " 100513512.9178123,\n",
       " 96490376.46768339,\n",
       " 87421689.2125995,\n",
       " 85059744.15252836,\n",
       " 82250041.36870164,\n",
       " 79337708.70513994,\n",
       " 76863524.38926047,\n",
       " 74604986.48410532,\n",
       " 71878524.11174372,\n",
       " 69184875.8128108,\n",
       " 727555046.326086,\n",
       " 705383548.071313,\n",
       " 684817341.7460083,\n",
       " 660730253.8628712,\n",
       " 635741118.7429016,\n",
       " 618642857.5972084,\n",
       " 601342055.7164099,\n",
       " 587562812.973239,\n",
       " 569553634.8959204,\n",
       " 555936212.2859966,\n",
       " 543755317.1413622,\n",
       " 531335564.0514238,\n",
       " 510841727.8999746,\n",
       " 491927488.82648635,\n",
       " 472074859.6405775,\n",
       " 440596949.9985708,\n",
       " 395204589.373084,\n",
       " 358067806.98171306,\n",
       " 336807112.3383714,\n",
       " 322733735.6789289,\n",
       " 309167740.319744,\n",
       " 294599304.18950146,\n",
       " 280059735.72384524,\n",
       " 266405715.21944284,\n",
       " 252090138.57646343,\n",
       " 244715146.59148872,\n",
       " 235508704.9094389,\n",
       " 228062557.51039812,\n",
       " 220074381.36481422,\n",
       " 212710548.88130185,\n",
       " 206114459.1695084,\n",
       " 199908499.61763152,\n",
       " 190199026.6178418,\n",
       " 187964004.65341768,\n",
       " 182543008.9582718,\n",
       " 176872514.16642702,\n",
       " 170819173.99537253,\n",
       " 164933311.60573047,\n",
       " 153820444.27445662,\n",
       " 146252856.54439706,\n",
       " 138422514.32523018,\n",
       " 128567444.45860945,\n",
       " 96626862.65115678,\n",
       " 104451169.94805752,\n",
       " 95438664.4681935,\n",
       " 682106610.688806,\n",
       " 669907154.0894043,\n",
       " 658944976.2353333,\n",
       " 646470180.5034548,\n",
       " 626693097.7066559,\n",
       " 621408811.558122,\n",
       " 608127817.7889731,\n",
       " 593924254.6460068,\n",
       " 583257033.8177439,\n",
       " 569137533.4063977,\n",
       " 556764506.5167289,\n",
       " 542370725.2233292,\n",
       " 525274262.36848134,\n",
       " 514332859.81645566,\n",
       " 490874688.69444865,\n",
       " 443279678.9446647,\n",
       " 425906974.7088394,\n",
       " 400967007.82308763,\n",
       " 361497846.0356452,\n",
       " 378246124.5556072,\n",
       " 341227765.96334624,\n",
       " 333412479.12877136,\n",
       " 328458832.3380743,\n",
       " 325154128.10226053,\n",
       " 319827473.9853354,\n",
       " 311720018.93878293,\n",
       " 305146407.024707,\n",
       " 297156959.01409245,\n",
       " 284552155.422003,\n",
       " 252474455.36220723,\n",
       " 260116894.4162028,\n",
       " 243521052.82500067,\n",
       " 241235179.8662697,\n",
       " 212534265.14769962,\n",
       " 243277094.49906325,\n",
       " 233994364.63402072,\n",
       " 185719726.64558557,\n",
       " 140192855.89834026,\n",
       " 121630108.16407375,\n",
       " 114087800.9732852,\n",
       " 102426312.78316006,\n",
       " 5476542.055954993,\n",
       " 684768401.2884369,\n",
       " 674041514.3448201,\n",
       " 663323894.4257609,\n",
       " 651487451.6588769,\n",
       " 637531965.747187,\n",
       " 617117322.0641707,\n",
       " 589917842.9796118,\n",
       " 576334931.6230694,\n",
       " 562836355.2431723,\n",
       " 553131099.7459928,\n",
       " 543417158.472795,\n",
       " 531961291.8460052,\n",
       " 522539547.599052,\n",
       " 515001243.0231063,\n",
       " 505618405.8028583,\n",
       " 496467860.2392062,\n",
       " 488115423.9355835,\n",
       " 478497437.1838764,\n",
       " 467512066.1489054,\n",
       " 454843978.41315514,\n",
       " 440430907.5283124,\n",
       " 425648032.918362,\n",
       " 415742000.3815001,\n",
       " 407045007.1234225,\n",
       " 395715546.25341386,\n",
       " 385409268.7945703,\n",
       " 372073227.66952765,\n",
       " 357417623.64319533,\n",
       " 346047519.45927644,\n",
       " 335728443.89511955,\n",
       " 326074299.0817529,\n",
       " 314742454.7656851,\n",
       " 305181358.4762059,\n",
       " 292266592.37454236,\n",
       " 280390127.7315749,\n",
       " 264754254.4143177,\n",
       " 254497176.78235283,\n",
       " 245098498.2988615,\n",
       " 236354951.8441765,\n",
       " 228819175.06554055,\n",
       " 223035451.438197,\n",
       " 213820401.5563712,\n",
       " 205112692.80667663,\n",
       " 194674409.18256548,\n",
       " 186481613.25991663,\n",
       " 156236676.8967247,\n",
       " 133624548.73394357,\n",
       " 125593191.73626833,\n",
       " 118992665.5003423,\n",
       " 110183847.01420793,\n",
       " 101616133.53572506,\n",
       " 97547179.37338687,\n",
       " 53124484.323325135,\n",
       " 44619457.01759579,\n",
       " 30595639.322155852,\n",
       " 31724437.356492467,\n",
       " 32194107.661763363,\n",
       " 28436557.57441923,\n",
       " 27749515.76649496,\n",
       " 679836107.2201252,\n",
       " 649256039.0399742,\n",
       " 620340372.1656287,\n",
       " 598974748.9251906,\n",
       " 586310148.0096018,\n",
       " 578840971.6879536,\n",
       " 560413501.9236516,\n",
       " 550742068.8037559,\n",
       " 537187145.1619583,\n",
       " 508786393.8937179,\n",
       " 474925560.40557814,\n",
       " 453119240.4079806,\n",
       " 440739027.4082004,\n",
       " 427562432.7995219,\n",
       " 412576139.91965777,\n",
       " 398584200.74040633,\n",
       " 390030178.9123695,\n",
       " 377844680.4934625,\n",
       " 358172577.61914444,\n",
       " 334873659.2101601,\n",
       " 257981008.3737906,\n",
       " 216245677.20631117,\n",
       " 198618116.17710453,\n",
       " 179420570.7499694,\n",
       " 160942960.93819383,\n",
       " 144024990.4874739,\n",
       " 12739205.856502304,\n",
       " 11781368.690416668,\n",
       " 10597151.74809439,\n",
       " 9791337.760604307,\n",
       " 8719117.189183291,\n",
       " 8390233.841191944,\n",
       " 7211198.011058317,\n",
       " 637984968.5406631,\n",
       " 611469946.5184023,\n",
       " 590926424.8521799,\n",
       " 576468321.3867735,\n",
       " 564442731.7089071,\n",
       " 550934057.0247868,\n",
       " 533850702.3792011,\n",
       " 512683119.47293156,\n",
       " 488778064.317548,\n",
       " 471351336.95298266,\n",
       " 453823910.9402633,\n",
       " 436183132.2090666,\n",
       " 413887100.87484354,\n",
       " 390066510.48450804,\n",
       " 364721009.1117766,\n",
       " 337184813.87328255,\n",
       " 307837703.7397917,\n",
       " 279374078.7490631,\n",
       " 262561641.65058836,\n",
       " 250180173.57595268,\n",
       " 239813998.54295576,\n",
       " 231256442.28690675,\n",
       " 224790700.87278628,\n",
       " 219209833.1384981,\n",
       " 211828563.76195765,\n",
       " 202536026.99019995,\n",
       " 192382832.19442794,\n",
       " 185746221.51728693,\n",
       " 179683454.3714661,\n",
       " 174093811.7958506,\n",
       " 168782370.85069016,\n",
       " 161730579.3276487,\n",
       " 156448040.57228774,\n",
       " 151056205.82135093,\n",
       " 146514849.98105678,\n",
       " 141860741.76014954,\n",
       " 135893222.49741045,\n",
       " 129469617.6140485,\n",
       " 123905765.52220741,\n",
       " 119144524.41907524,\n",
       " 114538131.51687835,\n",
       " 75957529.29160033,\n",
       " 70245363.38960104,\n",
       " 65355670.33469671,\n",
       " 60900710.60680892,\n",
       " 45584549.514620215,\n",
       " 42506925.42161509,\n",
       " 39672446.54053747,\n",
       " 37075200.36981837,\n",
       " 34543636.10202108,\n",
       " 31416434.990440644,\n",
       " 18317623.04005183,\n",
       " 635475032.1226795,\n",
       " 614634930.1263342,\n",
       " 620823059.4830931,\n",
       " 608868707.916173,\n",
       " 595108587.3915886,\n",
       " 577373907.6115079,\n",
       " 559980824.0555545,\n",
       " 535237378.8212951,\n",
       " 514351545.97099465,\n",
       " 499050928.88660884,\n",
       " 484576663.85873723,\n",
       " 469183379.1448272,\n",
       " 457974415.82065994,\n",
       " 444284977.1189178,\n",
       " 431303138.8779994,\n",
       " 415599320.2379401,\n",
       " 400768440.950056,\n",
       " 386105151.121191,\n",
       " 373419165.4007717,\n",
       " 360589700.3889569,\n",
       " 348644851.3375782,\n",
       " 339437335.04429173,\n",
       " 331447142.80903536,\n",
       " 323731569.0616517,\n",
       " 316137054.9326866,\n",
       " 308850178.3145971,\n",
       " 303412723.43516666,\n",
       " 298519377.2840222,\n",
       " 290629571.3346707,\n",
       " 284395646.76679206,\n",
       " 274267096.9435282,\n",
       " 263266841.50085443,\n",
       " 254480601.7241965,\n",
       " 244484932.6409261,\n",
       " 236263037.3695406,\n",
       " 226522386.92257354,\n",
       " 219336983.89033145,\n",
       " 213539595.9724696,\n",
       " 208846696.64010832,\n",
       " 200337889.15545747,\n",
       " 191253890.37600458,\n",
       " 183286769.3675757,\n",
       " 176020850.23206323,\n",
       " 170718383.9001027,\n",
       " 157693478.49815285,\n",
       " 143435330.298821,\n",
       " 137055144.28549507,\n",
       " 127307562.6178416,\n",
       " 99404658.72248736,\n",
       " 6971438.013070109,\n",
       " 603870588.7638305,\n",
       " 576136646.2187436,\n",
       " 551558665.8675573,\n",
       " 505493887.9293016,\n",
       " 469796070.73520327,\n",
       " 440829014.4932768,\n",
       " 330171670.7718059,\n",
       " 278605427.35211873,\n",
       " 254284168.4465223,\n",
       " 237939349.3104486,\n",
       " 218586172.35192326,\n",
       " 195284342.8905513,\n",
       " 156145461.578349,\n",
       " 133075584.23425765,\n",
       " 117971479.40631716,\n",
       " 101885589.69595791,\n",
       " 62805044.15449386,\n",
       " 60342707.874894135,\n",
       " 54565185.713383734,\n",
       " 52003102.364382096,\n",
       " 49530996.534910016,\n",
       " 46408225.239030816,\n",
       " 535076723.78247726,\n",
       " 490762884.1237889,\n",
       " 462109563.3138028,\n",
       " 445304331.8637541,\n",
       " 446545676.8104057,\n",
       " 428058620.34011227,\n",
       " 409900903.72235733,\n",
       " 391554795.3748205,\n",
       " 373468285.9535598,\n",
       " 318788071.71620375,\n",
       " 301670443.9258945,\n",
       " 283216979.9699991,\n",
       " 265480725.27433363,\n",
       " 241017785.1270005,\n",
       " 45661167.465707816,\n",
       " 44233484.543654196,\n",
       " 42809801.75640395,\n",
       " 41368858.736843154,\n",
       " 39946664.34239572,\n",
       " 38293309.702762164,\n",
       " 22839073.510946773,\n",
       " 20844806.703838978,\n",
       " 19461582.471856177,\n",
       " 18993821.560907505,\n",
       " 17896672.260371912,\n",
       " 17986864.06178562,\n",
       " 564927210.9667683,\n",
       " 542337821.0103164,\n",
       " 523327755.07895726,\n",
       " 504227075.1375136,\n",
       " 488605907.0949242,\n",
       " 473120579.4370979,\n",
       " 459267705.20948887,\n",
       " 438984130.72925776,\n",
       " 415458458.32094264,\n",
       " 391064731.68416023,\n",
       " 369537196.8118066,\n",
       " 351356084.1640295,\n",
       " 316789505.6981994,\n",
       " 287811516.54852754,\n",
       " 252384821.75963646,\n",
       " 219108677.0697925,\n",
       " 207836170.85711148,\n",
       " 208789062.74875927,\n",
       " 186944860.99242917,\n",
       " 181495003.33928606,\n",
       " 172748506.99129027,\n",
       " 171770242.284851,\n",
       " 169130812.7423768,\n",
       " 160439874.77999723,\n",
       " 162816001.52297708,\n",
       " 154348142.10289943,\n",
       " 147867381.883969,\n",
       " 146237084.40790066,\n",
       " 143704684.2037045,\n",
       " 142294384.2571518,\n",
       " 139156771.23788238,\n",
       " 134142675.183967,\n",
       " 134025309.26514253,\n",
       " 131257558.43666573,\n",
       " 123185578.04577409,\n",
       " 120179753.85124402,\n",
       " 115399144.46499126,\n",
       " 111062490.46003073,\n",
       " 108032783.14954178,\n",
       " 106021745.13930342,\n",
       " 101718665.37305456,\n",
       " 98190685.15801534,\n",
       " 95709233.74167527,\n",
       " 91126622.21742646,\n",
       " 85505487.18487546,\n",
       " 63184070.445291065,\n",
       " 59874080.62645459,\n",
       " 50141979.82205505,\n",
       " 47956213.797604755,\n",
       " 41270944.02235588,\n",
       " 31627517.193083648,\n",
       " 421352255.55361104,\n",
       " 399903728.10258085,\n",
       " 366184558.67080843,\n",
       " 370538333.6527665,\n",
       " 354738558.6303694,\n",
       " 320308789.913117,\n",
       " 296573634.8182566,\n",
       " 267823042.10689268,\n",
       " 553320726.1999558,\n",
       " 541741473.4465116,\n",
       " 532298325.48110586,\n",
       " 508084447.631259,\n",
       " 487875245.3268094,\n",
       " 468100947.834874,\n",
       " 452897906.31380945,\n",
       " 429486110.97986984,\n",
       " 378788398.9130336,\n",
       " 337180825.8607528,\n",
       " 311475766.0149303,\n",
       " 294277974.59362274,\n",
       " 279210026.52442837,\n",
       " 266883157.19539908,\n",
       " 257247678.81141794,\n",
       " 247229428.7375028,\n",
       " 238178377.45240712,\n",
       " 229653345.82568544,\n",
       " 221837960.23867866,\n",
       " 212361027.28729898,\n",
       " 204445227.95906025,\n",
       " 193652776.45557085,\n",
       " 183019914.51276004,\n",
       " 164163637.35711518,\n",
       " 153804279.21527198,\n",
       " 147334853.13771275,\n",
       " 142469124.9064653,\n",
       " 138621245.44029033,\n",
       " 134420961.5080122,\n",
       " 130837624.95321411,\n",
       " 127337489.83749339,\n",
       " 124479104.92646955,\n",
       " 121257522.54511787,\n",
       " 117969337.29704188,\n",
       " 112814661.23695634,\n",
       " 109514290.32389604,\n",
       " 106109205.47373796,\n",
       " 102189302.05012453,\n",
       " 97333150.67691149,\n",
       " 92107727.05069084,\n",
       " 88420995.29082324,\n",
       " 85232779.06922913,\n",
       " 10421245.86075858,\n",
       " 8628093.992415698,\n",
       " 6536209.371273282,\n",
       " 6363893.233952149,\n",
       " 438571096.7469636,\n",
       " 425743411.18914133,\n",
       " 413059619.5418318,\n",
       " 400886406.9426823,\n",
       " 374367890.2389157,\n",
       " 3382463.3749806555,\n",
       " 449244198.28710365,\n",
       " 425484365.35995835,\n",
       " 408471774.8564885,\n",
       " 396547744.32805437,\n",
       " 380576655.07261384,\n",
       " 367739249.70333284,\n",
       " 350566320.0847936,\n",
       " 343425259.68193626,\n",
       " 335215731.1852838,\n",
       " 306780913.7003696,\n",
       " 319844456.8324583,\n",
       " 312552160.89372724,\n",
       " 305609394.84595394,\n",
       " 299269173.6913742,\n",
       " 293336429.1456001,\n",
       " 287845646.0571526,\n",
       " 280725303.97652584,\n",
       " 273100264.35383123,\n",
       " 264841857.6910581,\n",
       " 247091739.99355063,\n",
       " 235103421.85799104,\n",
       " 218174540.8264825,\n",
       " 206704554.84750473,\n",
       " 193973593.08588022,\n",
       " 184566063.57805762,\n",
       " 64334708.12472493,\n",
       " 61427268.48865074,\n",
       " 58391414.96867565,\n",
       " 55495658.4538202,\n",
       " 52624971.96466834,\n",
       " 49060434.7128651,\n",
       " 36770146.36887627,\n",
       " 33527295.22322552,\n",
       " 28986794.695216473,\n",
       " 24693290.83965656,\n",
       " 16604941.016737388,\n",
       " 14507405.437561542,\n",
       " 13249415.929873971,\n",
       " 588688107.5627403,\n",
       " 579204032.7620506,\n",
       " 572047158.7136295,\n",
       " 543668996.3353543,\n",
       " 570656407.195565,\n",
       " 558310652.8650302,\n",
       " 545758359.0268146,\n",
       " 449056668.3067296,\n",
       " 432624205.22002405,\n",
       " 421629120.1956873,\n",
       " 411112517.09687495,\n",
       " 409628178.42243624,\n",
       " 417538225.3475302,\n",
       " 430510342.40238106,\n",
       " 405444701.0274227,\n",
       " 377107292.56812185,\n",
       " 374591460.30023897,\n",
       " 353703666.1256948,\n",
       " 219183442.10382727,\n",
       " 287222955.4727749,\n",
       " 278938182.73868805,\n",
       " 260016817.6108098,\n",
       " 172832418.7084627,\n",
       " 153239302.65173703,\n",
       " 124666142.73102845,\n",
       " 168504100.08936277,\n",
       " 155620290.06811896,\n",
       " 88217682.73142397,\n",
       " 137282823.84015158,\n",
       " 124460463.39798114,\n",
       " 118860755.94118997,\n",
       " 114370947.62407893,\n",
       " 109929689.06825007,\n",
       " 60270566.36719226,\n",
       " 43615634.23973977,\n",
       " 35823099.23680695,\n",
       " 51297230.116609134,\n",
       " 24540418.441489577,\n",
       " 24577625.370597843,\n",
       " 21356182.434175663,\n",
       " 458168578.06283295,\n",
       " 389229798.22725827,\n",
       " 362907036.41469026,\n",
       " 349995864.9090756,\n",
       " 342511623.66966003,\n",
       " 338145326.3909241,\n",
       " 328911406.8157051,\n",
       " 320595425.3685115,\n",
       " 308475910.6597892,\n",
       " 290185569.92867875,\n",
       " 279694287.871682,\n",
       " 270378830.7775077,\n",
       " 262179433.36133108,\n",
       " 254437545.37996316,\n",
       " 248011710.28065363,\n",
       " 242023978.13397214,\n",
       " 235192608.50351134,\n",
       " 228568299.10217214,\n",
       " 220899501.73327228,\n",
       " 214507624.28601074,\n",
       " 207458646.82253388,\n",
       " 203850246.11081582,\n",
       " 202361192.95044225,\n",
       " 199648734.53734544,\n",
       " 193934900.7305898,\n",
       " 187337234.5252334,\n",
       " 161374180.5826051,\n",
       " 177121639.55318037,\n",
       " 169496008.3124516,\n",
       " 166098884.09440386,\n",
       " 145580805.30043462,\n",
       " 153693826.12513763,\n",
       " 148606131.59177175,\n",
       " 140183196.8932611,\n",
       " 137526705.58991668,\n",
       " 128736468.8598001,\n",
       " 120783651.7837683,\n",
       " 474897351.59141046,\n",
       " 476474975.35194284,\n",
       " 463151169.373404,\n",
       " 409918061.81560177,\n",
       " 368905228.11646396,\n",
       " 364830998.88739014,\n",
       " 324152783.74982345,\n",
       " 282740446.0040756,\n",
       " 236616974.48368433,\n",
       " 265085119.43115634,\n",
       " 390847873.5992461,\n",
       " 353836437.0182077,\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2a185b8-418b-4736-a8fd-122c5b1248dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(fescs, bins = np.logspace(-6, 0))\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34014fdf-beb7-4e26-8621-2a37ae9dbc2d",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db24884c-0e8e-4767-9b58-499d05672d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys, os, time\n",
    "import optuna # Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5486470-f4c8-478c-a923-0aa5d3739ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /home/bsherwin51/anaconda3/lib/python3.10/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: numpy in /home/bsherwin51/anaconda3/lib/python3.10/site-packages (from optuna) (1.23.5)\n",
      "Collecting colorlog\n",
      "  Using cached colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Using cached alembic-1.11.1-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: tqdm in /home/bsherwin51/anaconda3/lib/python3.10/site-packages (from optuna) (4.64.1)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Using cached cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/bsherwin51/anaconda3/lib/python3.10/site-packages (from optuna) (1.4.39)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bsherwin51/anaconda3/lib/python3.10/site-packages (from optuna) (22.0)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/bsherwin51/anaconda3/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/bsherwin51/anaconda3/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/bsherwin51/anaconda3/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80d1523f-1697-4bc6-9dc9-b63808311a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hidden layer code\n",
    "\n",
    "# inp ---------> size of input data\n",
    "# h1 ----------> size of first hidden layer\n",
    "# out ---------> size of output data\n",
    "# dr ----------> dropout rate\n",
    "class model_1hl(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp, h1, out, dr):\n",
    "        super(model_1hl, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(inp, h1) # lin trans\n",
    "        self.fc2 = nn.Linear(h1,  out)\n",
    "        \n",
    "        self.dropout   = nn.Dropout(p=dr) #normalizes by zeroing at random\n",
    "        self.ReLU      = nn.ReLU() # RELU(x) = max(0,x)\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2) # LeakyReLU = max(0,x) + neg_slope * min(0,x)\n",
    "        \n",
    "        # initialize the weights of the different layers\n",
    "        for m in self.modules(): \n",
    "            if isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm1d): # if module is Batch normalization: y = gamma*(x -  E(x))/sqrt(Var(x) + eps) + b\n",
    "                nn.init.constant_(m.weight, 1) \n",
    "                nn.init.constant_(m.bias, 1)\n",
    "            elif isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "                 '''if module is 3D Convolution: over an input signal composed of several input planes. or ConvTranspose2d Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes. Or Lin Trans'''\n",
    "                 nn.init.kaiming_normal_(m.weight) # normalization that makes extremely deep models (>30 layers) to converge\n",
    "       \n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.LeakyReLU(self.fc1(x))) # Apply leakyRELU on input and dropout normalize\n",
    "        out = self.fc2(out) \n",
    "        # out = nn.Softmax(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d99dc4a-f01c-497e-9f86-f9360447c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N hidden layer code, similar structure to above but generalized\n",
    "def dynamic_model2(input_size, output_size, n_layers, hidden, dr):\n",
    "\n",
    "    # define the tuple containing the different layers\n",
    "    layers = []\n",
    "\n",
    "    # get the hidden layers\n",
    "    in_features = input_size\n",
    "    for i in range(n_layers):\n",
    "        out_features = hidden[i]\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        layers.append(nn.Dropout(dr[i]))\n",
    "        in_features = out_features\n",
    "\n",
    "    # get the last layer\n",
    "    layers.append(nn.Linear(out_features, output_size))\n",
    "\n",
    "    # return the model\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "#model = dynamic_model2(numFeatures, outSize, numHL, [h1, h2], [dr1, dr1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49141d45-08dc-4cc1-9012-9c1473463d74",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f54421c7-c467-41d3-ab5d-ddd402291774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import torch \n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import sys, os, time, h5py\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d149b79d-3203-4d49-94b6-9836dcb315d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, labels):\n",
    "    ######################\n",
    "    # normalize input\n",
    "    ## the id's correspond to the id's of the feature array you made using vstack in read_data()\n",
    "    data[:,0]  = (data[:,0] - np.mean(data[:,0]))/np.std(data[:,0]) # Z Scores\n",
    "    data[:,1]  = (data[:,1] - np.mean(data[:,1]))/np.std(data[:,1])\n",
    "    data[:,2]  = (data[:,2] - np.mean(data[:,2]))/np.std(data[:,2])\n",
    "    data[:,3]  = (data[:,3] - np.mean(data[:,3]))/np.std(data[:,3])\n",
    "    data[:,4]  = (data[:,4] - np.mean(data[:,4]))/np.std(data[:,4])\n",
    "\n",
    "    ######################\n",
    "    # normalize labels\n",
    "    print (\"labels\", labels)\n",
    "\n",
    "    # # array = np.log10(labels) # Since labels are large log them\n",
    "    # print(\"labels array\", array)\n",
    "    # labels = array\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "feea7823-2bb5-4714-934c-4d94e00cd77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and get training, validation or testing sets\n",
    "# fin ---------> file with the data\n",
    "# seed --------> random seed used to split among different datasets\n",
    "# mode --------> 'train', 'valid', 'test' or 'all'\n",
    "# normalize ---> whether to normalize the data or not\n",
    "def read_data(fin, seed, mode, normalize): #fin for h5\n",
    "    File = 'catalog.h5'\n",
    "    f     = h5py.File(fin, 'r')\n",
    "    masses = []\n",
    "    fgases = []\n",
    "    SFRs = []\n",
    "    Mstars = []\n",
    "    redshifts = []\n",
    "    fescs = []\n",
    "    groups = []\n",
    "    min_dist = []\n",
    "    t_lookback = []\n",
    "    \n",
    "    \n",
    "    t_lookback_normal = np.array(np.load('t_lookback_normal.npy', allow_pickle = True))\n",
    "    t_lookback_all = []\n",
    "    for list_i in t_lookback_normal:\n",
    "        for elem in list_i:\n",
    "            t_lookback_all.append(elem)\n",
    "        \n",
    "    min_dist_normal = np.array(np.load('min_dist_normal.npy', allow_pickle = True))\n",
    "    min_dist_all = []\n",
    "    for listi in min_dist_normal:\n",
    "        for elem in listi:\n",
    "            min_dist_all.append(elem)\n",
    "    \n",
    "    for halo in f.keys():\n",
    "        fesc_list = list(f[halo]['fesc']) \n",
    "        for index, fesc in enumerate(fesc_list):\n",
    "            if fesc <= 0.01:\n",
    "                group = 0\n",
    "            elif 0.01 < fesc <= 0.10:\n",
    "                group = 1\n",
    "            elif 0.10 < fesc <= 0.25:\n",
    "                group = 2\n",
    "            else:\n",
    "                group = 3\n",
    "                \n",
    "            if fesc >= 1e-5:\n",
    "                masses.append(f[halo]['mass'][index])\n",
    "                fgases.append(f[halo]['fgas'][index])\n",
    "                SFRs.append(f[halo]['SFR'][index])\n",
    "                Mstars.append(f[halo]['Mstar'][index])\n",
    "                redshifts.append(f[halo]['redshift'][index])\n",
    "                fescs.append(fesc)\n",
    "                groups.append(group)\n",
    "                t_lookback.append(t_lookback_all[index])\n",
    "                min_dist.append(min_dist_all[index])\n",
    "                \n",
    "    # min_SFR = np.min(SFRs[SFRs != 0])\n",
    "    minsfr = 100\n",
    "    for each in SFRs:\n",
    "        if each < minsfr and each != 0.:\n",
    "            minsfr = each\n",
    "    new_SFRs = []\n",
    "    for SFR in SFRs:\n",
    "        if SFR == 0.0:\n",
    "            new_SFRs.append(minsfr*0.9)\n",
    "        else:\n",
    "            new_SFRs.append(SFR)        \n",
    "    # normalize data - EDIT\n",
    "    ## anything not on order 10^0 - 10^1 should be logged\n",
    "    masses = np.log10(masses)\n",
    "    SFRs = np.log10(new_SFRs)\n",
    "    Mstars = np.log10(Mstars)\n",
    "    fgases = np.log10(fgases)\n",
    "    min_dist = np.log10(min_dist)\n",
    "    t_lookback = np.log10(t_lookback)\n",
    "    \n",
    "    # get data, labels and number of elements\n",
    "    data = np.vstack([SFRs, Mstars, masses, redshifts, fgases, min_dist, t_lookback]).T # THESE ARE YOUR CHOSEN INPUT VARIABLES\n",
    "    \n",
    "    labels = np.array(groups, dtype = 'int')\n",
    "    print(\"labels.shape\", labels.shape)\n",
    "    # labels = fluxes.reshape((fluxes.shape[0], fluxSize))\n",
    "    elements = data.shape[0]\n",
    "    \n",
    "    # normalize data\n",
    "    if normalize:  data, labels = normalize_data(data, labels)\n",
    "\n",
    "    # get the size and offset depending on the type of dataset\n",
    "    if   mode=='train':   size, offset = int(elements*0.70), int(elements*0.00)\n",
    "    elif mode=='valid':   size, offset = int(elements*0.15), int(elements*0.70)\n",
    "    elif mode=='test':    size, offset = int(elements*0.15), int(elements*0.85)\n",
    "    elif mode=='all':     size, offset = int(elements*1.00), int(elements*0.00)\n",
    "    else:                 raise Exception('Wrong name!')\n",
    "\n",
    "    # randomly shuffle the cubes. Instead of 0 1 2 3...999 have a \n",
    "    # random permutation. E.g. 5 9 0 29...342\n",
    "    np.random.seed(seed)\n",
    "    indexes = np.arange(elements) \n",
    "    np.random.shuffle(indexes)\n",
    "    indexes = indexes[offset:offset+size] #select indexes of mode\n",
    "\n",
    "    return data[indexes], labels[indexes]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "265177a9-444b-47b3-bf74-3f12859100c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_dataset():\n",
    "\n",
    "    def __init__(self, mode, seed, fin):\n",
    "\n",
    "        # get data\n",
    "        inp, out = read_data(fin, seed, mode, normalize=True)\n",
    "\n",
    "        # get the corresponding bottlenecks and parameters\n",
    "        self.size   = inp.shape[0]\n",
    "        self.input  = torch.tensor(inp, dtype=torch.float32)\n",
    "        self.output = torch.tensor(out, dtype= torch.long)\n",
    "        \n",
    "        print (\"size of input and output\", np.shape(self.input), np.shape(self.output))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.output[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "179771f3-be94-4dd8-b41e-4db2d4e6452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(mode, seed, fin, batch_size, shuffle):\n",
    "    data_set = make_dataset(mode, seed, fin)\n",
    "    dataset_loader = DataLoader(dataset=data_set, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataset_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e53da-8d0b-490e-bc6f-24370b96e0c3",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5616552d-a48d-42c8-8e54-a14a02fe4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = 'catalog.h5'\n",
    "dataname = 'Normal'      # Nickname for data\n",
    "seed = 20                # remember this to reproduce split\n",
    "numFeatures = 7        # number of input properties\n",
    "outSize = 4          # size of wavelength array\n",
    "featurelist = 'SFRs, Mstars, masses, redshifts, fgases, min_dist, t_lookback'\n",
    "\n",
    "numHL = 5           # number of hidden layers\n",
    "# h1 = 954                # nodes in first hidden layer\n",
    "h1_1 = 265            # nodes in second hidden layer\n",
    "h2_1 = 306\n",
    "h3_1 = 286\n",
    "h4_1 = 309\n",
    "h5_1 = 499\n",
    "dr1_1 = 0.48303759321338957\n",
    "dr2_1 = 0.32620849779889893\n",
    "dr3_1 = 0.3163743611025014\n",
    "dr4_1 = 0.7661053540089837\n",
    "dr5_1 = 0.3339235798444601\n",
    "lr_1 = 0.0013378392548823155\n",
    "wd_1 = 1.033893175510939e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5f76e-4f20-4611-8a50-2a41e6c5bf0f",
   "metadata": {},
   "source": [
    "Trial 46 finished with value: 0.8143988083570431 and parameters: {'dropout_l0': 0.48303759321338957, 'dropout_l1': 0.32620849779889893, 'dropout_l2': 0.3163743611025014, 'dropout_l3': 0.7661053540089837, 'dropout_l4': 0.3339235798444601, 'lr': 0.0013378392548823155, 'n_layers': 5, 'n_units_l0': 265, 'n_units_l1': 306, 'n_units_l2': 286, 'n_units_l3': 309, 'n_units_l4': 499, 'wd': 1.033893175510939e-05}. Best is trial 46 with value: 0.8143988083570431."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37491d12-4964-43d3-9f23-1c49e7e0e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "numHL = 4           # number of hidden layers\n",
    "# h1 = 954                # nodes in first hidden layer\n",
    "h1_2 = 319            # nodes in second hidden layer\n",
    "h2_2 = 309\n",
    "h3_2 = 487\n",
    "h4_2 = 245\n",
    "dr1_2 = 0.48326007288595463\n",
    "dr2_2 = 0.22691770014786583\n",
    "dr3_2 = 0.4133618765743943\n",
    "dr4_2 = 0.20222387654738752\n",
    "lr_2 = 0.0024461318194048767\n",
    "wd_2 = 0.00038150231346785777"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f2120-c2c2-4bf4-961b-715b292ee75c",
   "metadata": {},
   "source": [
    "Trial 38 finished with value: 0.8062396895172249 and parameters: {'n_layers': 4, 'n_units_l0': 319, 'dropout_l0': 0.48326007288595463, 'n_units_l1': 309, 'dropout_l1': 0.22691770014786583, 'n_units_l2': 487, 'dropout_l2': 0.4133618765743943, 'n_units_l3': 245, 'dropout_l3': 0.20222387654738752, 'lr': 0.0024461318194048767, 'wd': 0.00038150231346785777}. Best is trial 38 with value: 0.8062396895172249."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f04cb86-18c1-4c23-a3a5-4482d6ca6fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numHL = 4           # number of hidden layers\n",
    "# h1 = 954                # nodes in first hidden layer\n",
    "h1_3 = 360            # nodes in second hidden layer\n",
    "h2_3 = 318\n",
    "h3_3 = 186\n",
    "h4_3 = 230\n",
    "dr1_3 = 0.5991490308822351\n",
    "dr2_3 = 0.6557328854093288\n",
    "dr3_3 = 0.2229397980059654\n",
    "dr4_3 = 0.37167582651003706\n",
    "lr_3 = 0.0005674724247255081\n",
    "wd_3 = 4.344007387477737e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0153b49-e00f-43df-8039-b424fc313187",
   "metadata": {},
   "source": [
    "Trial 19 finished with value: 0.810626839980101 and parameters: {'n_layers': 4, 'n_units_l0': 360, 'dropout_l0': 0.5991490308822351, 'n_units_l1': 318, 'dropout_l1': 0.6557328854093288, 'n_units_l2': 186, 'dropout_l2': 0.2229397980059654, 'n_units_l3': 230, 'dropout_l3': 0.37167582651003706, 'lr': 0.0005674724247255081, 'wd': 4.344007387477737e-05}. Best is trial 19 with value: 0.810626839980101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "152809e4-9d74-479f-b4ae-c41966257078",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_non = 150\n",
    "h2_non = 150\n",
    "numHL_non = 2\n",
    "# # # dr1 = 0.20103              # dropout rate for layer 1\n",
    "dr1_non = 0.35\n",
    "# # # training parameters\n",
    "batch_size = 256  \n",
    "# # # lr         = 1.404e-3     (learning rate)\n",
    "lr_non = 0.01\n",
    "epochs     = 1000\n",
    "# # # wd         = 1.195e-6\n",
    "wd_non = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9e3f5-7ebe-459e-8dd4-56a334472c16",
   "metadata": {},
   "source": [
    "Trial 49 finished with value: 0.7992990006748427 and parameters: {'n_layers': 4, 'n_units_l0': 326, 'dropout_l0': 0.6140597590944773, 'n_units_l1': 203, 'dropout_l1': 0.28132639142015986, 'n_units_l2': 460, 'dropout_l2': 0.25855030410354674, 'n_units_l3': 337, 'dropout_l3': 0.24732580986058228, 'lr': 0.0033140422147764532, 'wd': 0.0020272408464552176}. Best is trial 49 with value: 0.7992990006748427."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "378350be-b1d1-46a3-b82b-a9d603e8e513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing dataset...\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    }
   ],
   "source": [
    "# name of output files\n",
    "\n",
    "\n",
    "name   = \"Normal_dynamicmodel2\"\n",
    "device = torch.device('cpu')\n",
    "# define loss function\n",
    "criterion = nn.CrossEntropyLoss()  # MSE or Cross Entropy\n",
    "\n",
    "# get train, validation, and test sets\n",
    "print('preparing dataset...')\n",
    "train_loader = create_dataset('train', seed, fin, batch_size, True)\n",
    "valid_loader = create_dataset('valid', seed, fin, batch_size, False)\n",
    "test_loader  = create_dataset('test',  seed, fin, batch_size, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82c7df9e-ea2d-4dc3-8870-9bae4889fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296,)\n",
      "(3122,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(min_dist))\n",
    "print(np.shape(SFRs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee349a-1127-4b23-a092-3731d284debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(masses).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0135143-e603-4b72-8cc4-288bf94a78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define architecture\n",
    "# model = architecture.model_1hl(numFeatures, h1, fluxSize, dr1) # change function according to number of hidden layers\n",
    "model = dynamic_model2(numFeatures, outSize, numHL, [h1_1, h2_1, h3_1, h4_1, h5_1], [dr1_1, dr2_1, dr3_1, dr4_1, dr5_1])\n",
    "#model_non = dynamic_model2(numFeatures, outSize, numHL_non, [h1_non, h2_non], [dr1_non, dr1_non])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "844d4548-5f76-429f-95ae-ff7549f6f3f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of parameters in the model = 416691\n",
      "000 1.330e+00 1.302e+00 (saving)\n",
      "001 1.253e+00 1.199e+00 (saving)\n",
      "002 1.159e+00 1.120e+00 (saving)\n",
      "003 1.134e+00 1.082e+00 (saving)\n",
      "004 1.162e+00 1.070e+00 (saving)\n",
      "005 1.096e+00 1.066e+00 (saving)\n",
      "006 1.076e+00 1.154e+00\n",
      "007 1.089e+00 1.059e+00 (saving)\n",
      "008 1.065e+00 1.052e+00 (saving)\n",
      "009 1.064e+00 1.053e+00\n",
      "010 1.055e+00 1.029e+00 (saving)\n",
      "011 1.045e+00 1.030e+00\n",
      "012 1.055e+00 1.025e+00 (saving)\n",
      "013 1.041e+00 1.132e+00\n",
      "014 1.042e+00 1.036e+00\n",
      "015 1.017e+00 1.038e+00\n",
      "016 1.020e+00 1.016e+00 (saving)\n",
      "017 1.010e+00 1.028e+00\n",
      "018 1.020e+00 1.075e+00\n",
      "019 1.011e+00 9.848e-01 (saving)\n",
      "020 1.019e+00 1.002e+00\n",
      "021 1.007e+00 1.060e+00\n",
      "022 1.002e+00 1.031e+00\n",
      "023 1.000e+00 9.865e-01\n",
      "024 1.005e+00 9.948e-01\n",
      "025 9.891e-01 9.936e-01\n",
      "026 1.007e+00 1.007e+00\n",
      "027 9.876e-01 9.767e-01 (saving)\n",
      "028 9.897e-01 9.931e-01\n",
      "029 9.957e-01 9.980e-01\n",
      "030 9.854e-01 1.003e+00\n",
      "031 9.814e-01 9.767e-01 (saving)\n",
      "032 9.908e-01 9.804e-01\n",
      "033 9.888e-01 9.960e-01\n",
      "034 9.700e-01 9.858e-01\n",
      "035 9.781e-01 9.780e-01\n",
      "036 9.587e-01 1.008e+00\n",
      "037 9.675e-01 9.644e-01 (saving)\n",
      "038 9.715e-01 1.012e+00\n",
      "039 9.690e-01 9.957e-01\n",
      "040 9.652e-01 9.915e-01\n",
      "041 9.825e-01 9.609e-01 (saving)\n",
      "042 9.475e-01 9.623e-01\n",
      "043 9.818e-01 9.813e-01\n",
      "044 9.633e-01 9.712e-01\n",
      "045 9.616e-01 9.636e-01\n",
      "046 9.634e-01 9.793e-01\n",
      "047 9.759e-01 9.866e-01\n",
      "048 9.662e-01 9.636e-01\n",
      "049 9.574e-01 9.580e-01 (saving)\n",
      "050 9.481e-01 9.598e-01\n",
      "051 9.490e-01 9.940e-01\n",
      "052 9.544e-01 9.501e-01 (saving)\n",
      "053 9.541e-01 9.834e-01\n",
      "054 9.462e-01 9.587e-01\n",
      "055 9.468e-01 9.616e-01\n",
      "056 9.528e-01 1.002e+00\n",
      "057 9.565e-01 9.560e-01\n",
      "058 9.446e-01 9.714e-01\n",
      "059 9.358e-01 9.717e-01\n",
      "060 9.471e-01 1.018e+00\n",
      "061 9.441e-01 9.525e-01\n",
      "062 9.375e-01 9.537e-01\n",
      "063 9.362e-01 9.558e-01\n",
      "064 9.302e-01 9.498e-01 (saving)\n",
      "065 9.465e-01 9.461e-01 (saving)\n",
      "066 9.452e-01 9.703e-01\n",
      "067 9.324e-01 9.799e-01\n",
      "068 9.357e-01 9.539e-01\n",
      "069 9.455e-01 9.444e-01 (saving)\n",
      "070 9.451e-01 1.029e+00\n",
      "071 9.529e-01 9.614e-01\n",
      "072 9.328e-01 9.468e-01\n",
      "073 9.421e-01 9.511e-01\n",
      "074 9.390e-01 9.443e-01 (saving)\n",
      "075 9.316e-01 9.659e-01\n",
      "076 9.221e-01 9.457e-01\n",
      "077 9.244e-01 9.446e-01\n",
      "078 9.234e-01 9.476e-01\n",
      "079 9.308e-01 9.783e-01\n",
      "080 9.339e-01 9.413e-01 (saving)\n",
      "081 9.179e-01 9.700e-01\n",
      "082 9.253e-01 9.755e-01\n",
      "083 9.394e-01 9.740e-01\n",
      "084 9.302e-01 1.031e+00\n",
      "085 9.467e-01 9.405e-01 (saving)\n",
      "086 9.249e-01 9.456e-01\n",
      "087 9.178e-01 9.553e-01\n",
      "088 9.318e-01 9.444e-01\n",
      "089 9.335e-01 9.458e-01\n",
      "090 9.261e-01 9.480e-01\n",
      "091 9.162e-01 9.507e-01\n",
      "092 9.140e-01 9.369e-01 (saving)\n",
      "093 9.236e-01 9.356e-01 (saving)\n",
      "094 9.182e-01 9.412e-01\n",
      "095 9.176e-01 9.333e-01 (saving)\n",
      "096 9.203e-01 9.448e-01\n",
      "097 9.150e-01 9.510e-01\n",
      "098 9.280e-01 9.376e-01\n",
      "099 9.269e-01 9.432e-01\n",
      "100 9.226e-01 9.371e-01\n",
      "101 9.107e-01 9.412e-01\n",
      "102 9.184e-01 9.479e-01\n",
      "103 9.181e-01 9.806e-01\n",
      "104 9.164e-01 9.373e-01\n",
      "105 9.091e-01 9.498e-01\n",
      "106 9.240e-01 9.445e-01\n",
      "107 9.180e-01 9.374e-01\n",
      "108 9.146e-01 9.491e-01\n",
      "109 9.012e-01 9.438e-01\n",
      "110 9.000e-01 9.365e-01\n",
      "111 9.056e-01 9.353e-01\n",
      "112 9.271e-01 9.432e-01\n",
      "113 9.233e-01 9.323e-01 (saving)\n",
      "114 9.040e-01 9.338e-01\n",
      "115 8.953e-01 9.297e-01 (saving)\n",
      "116 9.093e-01 9.332e-01\n",
      "117 9.050e-01 9.513e-01\n",
      "118 9.032e-01 9.422e-01\n",
      "119 9.102e-01 9.381e-01\n",
      "120 9.115e-01 9.340e-01\n",
      "121 9.063e-01 9.283e-01 (saving)\n",
      "122 9.027e-01 9.350e-01\n",
      "123 9.093e-01 9.283e-01\n",
      "124 9.084e-01 9.381e-01\n",
      "125 9.061e-01 9.346e-01\n",
      "126 9.078e-01 9.287e-01\n",
      "127 9.001e-01 9.335e-01\n",
      "128 9.144e-01 9.394e-01\n",
      "129 9.045e-01 9.413e-01\n",
      "130 9.017e-01 9.318e-01\n",
      "131 9.017e-01 9.263e-01 (saving)\n",
      "132 8.837e-01 9.257e-01 (saving)\n",
      "133 8.991e-01 9.222e-01 (saving)\n",
      "134 8.900e-01 9.230e-01\n",
      "135 9.071e-01 9.321e-01\n",
      "136 9.120e-01 9.336e-01\n",
      "137 9.068e-01 9.412e-01\n",
      "138 8.984e-01 9.291e-01\n",
      "139 9.074e-01 9.315e-01\n",
      "140 9.004e-01 9.290e-01\n",
      "141 8.840e-01 9.192e-01 (saving)\n",
      "142 8.941e-01 9.414e-01\n",
      "143 9.057e-01 9.267e-01\n",
      "144 8.927e-01 9.227e-01\n",
      "145 9.025e-01 9.206e-01\n",
      "146 8.966e-01 9.429e-01\n",
      "147 9.016e-01 9.325e-01\n",
      "148 8.895e-01 9.314e-01\n",
      "149 8.938e-01 9.216e-01\n",
      "150 8.993e-01 9.255e-01\n",
      "151 8.787e-01 9.220e-01\n",
      "152 8.931e-01 9.216e-01\n",
      "153 8.990e-01 9.461e-01\n",
      "154 8.991e-01 9.150e-01 (saving)\n",
      "155 8.921e-01 9.434e-01\n",
      "156 8.870e-01 9.204e-01\n",
      "157 8.934e-01 9.162e-01\n",
      "158 8.961e-01 9.218e-01\n",
      "159 8.880e-01 9.213e-01\n",
      "160 8.876e-01 9.213e-01\n",
      "161 8.848e-01 9.143e-01 (saving)\n",
      "162 8.961e-01 9.270e-01\n",
      "163 8.924e-01 9.571e-01\n",
      "164 8.902e-01 9.199e-01\n",
      "165 8.937e-01 9.091e-01 (saving)\n",
      "166 8.883e-01 9.157e-01\n",
      "167 8.963e-01 9.164e-01\n",
      "168 8.750e-01 9.132e-01\n",
      "169 8.931e-01 9.104e-01\n",
      "170 8.916e-01 9.213e-01\n",
      "171 8.909e-01 9.154e-01\n",
      "172 8.844e-01 9.377e-01\n",
      "173 8.935e-01 9.441e-01\n",
      "174 8.953e-01 9.100e-01\n",
      "175 8.879e-01 9.063e-01 (saving)\n",
      "176 8.852e-01 9.145e-01\n",
      "177 8.960e-01 9.306e-01\n",
      "178 8.916e-01 9.087e-01\n",
      "179 8.961e-01 9.356e-01\n",
      "180 8.891e-01 9.214e-01\n",
      "181 8.831e-01 9.181e-01\n",
      "182 8.758e-01 9.175e-01\n",
      "183 8.778e-01 9.194e-01\n",
      "184 8.860e-01 9.202e-01\n",
      "185 8.803e-01 9.163e-01\n",
      "186 8.701e-01 9.156e-01\n",
      "187 8.899e-01 9.026e-01 (saving)\n",
      "188 8.749e-01 9.107e-01\n",
      "189 8.819e-01 9.026e-01 (saving)\n",
      "190 8.904e-01 9.365e-01\n",
      "191 8.839e-01 9.372e-01\n",
      "192 8.923e-01 9.076e-01\n",
      "193 8.674e-01 9.115e-01\n",
      "194 8.886e-01 9.003e-01 (saving)\n",
      "195 8.807e-01 9.110e-01\n",
      "196 8.720e-01 9.189e-01\n",
      "197 8.914e-01 9.103e-01\n",
      "198 8.689e-01 9.036e-01\n",
      "199 8.775e-01 9.152e-01\n",
      "200 8.868e-01 9.111e-01\n",
      "201 8.810e-01 9.196e-01\n",
      "202 8.793e-01 9.029e-01\n",
      "203 8.786e-01 9.223e-01\n",
      "204 8.833e-01 9.113e-01\n",
      "205 8.784e-01 9.114e-01\n",
      "206 8.793e-01 9.055e-01\n",
      "207 8.789e-01 9.096e-01\n",
      "208 8.839e-01 9.151e-01\n",
      "209 8.717e-01 9.338e-01\n",
      "210 8.716e-01 9.056e-01\n",
      "211 8.649e-01 9.204e-01\n",
      "212 8.862e-01 9.119e-01\n",
      "213 8.789e-01 9.170e-01\n",
      "214 8.793e-01 9.039e-01\n",
      "215 8.726e-01 9.217e-01\n",
      "216 8.830e-01 9.147e-01\n",
      "217 8.717e-01 9.334e-01\n",
      "218 8.613e-01 9.090e-01\n",
      "219 8.717e-01 9.176e-01\n",
      "220 8.761e-01 9.009e-01\n",
      "221 8.714e-01 9.086e-01\n",
      "222 8.757e-01 9.416e-01\n",
      "223 8.706e-01 9.012e-01\n",
      "224 8.715e-01 9.128e-01\n",
      "225 8.716e-01 9.232e-01\n",
      "226 8.702e-01 9.188e-01\n",
      "227 8.656e-01 9.097e-01\n",
      "228 8.725e-01 9.132e-01\n",
      "229 8.680e-01 9.082e-01\n",
      "230 8.758e-01 9.282e-01\n",
      "231 8.639e-01 9.033e-01\n",
      "232 8.712e-01 9.005e-01\n",
      "233 8.614e-01 9.468e-01\n",
      "234 8.782e-01 9.019e-01\n",
      "235 8.688e-01 9.157e-01\n",
      "236 8.827e-01 9.125e-01\n",
      "237 8.722e-01 8.981e-01 (saving)\n",
      "238 8.670e-01 9.082e-01\n",
      "239 8.701e-01 9.126e-01\n",
      "240 8.652e-01 9.090e-01\n",
      "241 8.648e-01 9.446e-01\n",
      "242 8.762e-01 9.132e-01\n",
      "243 8.677e-01 9.050e-01\n",
      "244 8.618e-01 9.215e-01\n",
      "245 8.596e-01 9.105e-01\n",
      "246 8.702e-01 9.279e-01\n",
      "247 8.645e-01 9.039e-01\n",
      "248 8.628e-01 9.073e-01\n",
      "249 8.708e-01 9.326e-01\n",
      "250 8.779e-01 8.946e-01 (saving)\n",
      "251 8.647e-01 9.336e-01\n",
      "252 8.564e-01 9.054e-01\n",
      "253 8.710e-01 9.021e-01\n",
      "254 8.661e-01 9.329e-01\n",
      "255 8.742e-01 9.047e-01\n",
      "256 8.604e-01 9.032e-01\n",
      "257 8.682e-01 8.998e-01\n",
      "258 8.623e-01 9.455e-01\n",
      "259 8.747e-01 9.026e-01\n",
      "260 8.768e-01 8.962e-01\n",
      "261 8.569e-01 9.045e-01\n",
      "262 8.704e-01 9.263e-01\n",
      "263 8.639e-01 9.231e-01\n",
      "264 8.624e-01 9.152e-01\n",
      "265 8.608e-01 8.999e-01\n",
      "266 8.669e-01 8.937e-01 (saving)\n",
      "267 8.711e-01 8.976e-01\n",
      "268 8.600e-01 9.309e-01\n",
      "269 8.621e-01 9.267e-01\n",
      "270 8.684e-01 9.167e-01\n",
      "271 8.505e-01 9.046e-01\n",
      "272 8.674e-01 9.036e-01\n",
      "273 8.610e-01 9.227e-01\n",
      "274 8.713e-01 9.045e-01\n",
      "275 8.702e-01 9.014e-01\n",
      "276 8.467e-01 9.059e-01\n",
      "277 8.492e-01 9.126e-01\n",
      "278 8.597e-01 9.031e-01\n",
      "279 8.554e-01 8.981e-01\n",
      "280 8.512e-01 9.000e-01\n",
      "281 8.686e-01 9.123e-01\n",
      "282 8.622e-01 9.297e-01\n",
      "283 8.686e-01 9.051e-01\n",
      "284 8.562e-01 9.356e-01\n",
      "285 8.575e-01 9.210e-01\n",
      "286 8.637e-01 9.027e-01\n",
      "287 8.732e-01 9.265e-01\n",
      "288 8.562e-01 9.055e-01\n",
      "289 8.473e-01 8.950e-01\n",
      "290 8.650e-01 9.172e-01\n",
      "291 8.511e-01 8.967e-01\n",
      "292 8.645e-01 9.120e-01\n",
      "293 8.671e-01 8.925e-01 (saving)\n",
      "294 8.434e-01 9.068e-01\n",
      "295 8.530e-01 9.195e-01\n",
      "296 8.557e-01 9.081e-01\n",
      "297 8.618e-01 9.084e-01\n",
      "298 8.655e-01 8.966e-01\n",
      "299 8.548e-01 9.105e-01\n",
      "300 8.567e-01 8.994e-01\n",
      "301 8.496e-01 9.131e-01\n",
      "302 8.541e-01 8.938e-01\n",
      "303 8.608e-01 9.007e-01\n",
      "304 8.586e-01 9.181e-01\n",
      "305 8.559e-01 8.944e-01\n",
      "306 8.544e-01 9.279e-01\n",
      "307 8.450e-01 9.036e-01\n",
      "308 8.346e-01 9.244e-01\n",
      "309 8.617e-01 8.955e-01\n",
      "310 8.586e-01 9.169e-01\n",
      "311 8.571e-01 9.082e-01\n",
      "312 8.594e-01 9.103e-01\n",
      "313 8.549e-01 9.072e-01\n",
      "314 8.614e-01 9.138e-01\n",
      "315 8.487e-01 9.138e-01\n",
      "316 8.514e-01 8.978e-01\n",
      "317 8.647e-01 9.058e-01\n",
      "318 8.516e-01 9.002e-01\n",
      "319 8.470e-01 8.976e-01\n",
      "320 8.487e-01 9.041e-01\n",
      "321 8.512e-01 9.272e-01\n",
      "322 8.490e-01 9.049e-01\n",
      "323 8.547e-01 9.266e-01\n",
      "324 8.589e-01 8.994e-01\n",
      "325 8.578e-01 9.330e-01\n",
      "326 8.519e-01 9.018e-01\n",
      "327 8.545e-01 9.148e-01\n",
      "328 8.391e-01 9.013e-01\n",
      "329 8.499e-01 9.085e-01\n",
      "330 8.385e-01 9.059e-01\n",
      "331 8.425e-01 8.998e-01\n",
      "332 8.364e-01 9.201e-01\n",
      "333 8.577e-01 9.175e-01\n",
      "334 8.523e-01 9.039e-01\n",
      "335 8.443e-01 8.960e-01\n",
      "336 8.591e-01 9.199e-01\n",
      "337 8.462e-01 9.051e-01\n",
      "338 8.466e-01 9.118e-01\n",
      "339 8.525e-01 9.002e-01\n",
      "340 8.510e-01 9.099e-01\n",
      "341 8.478e-01 8.923e-01 (saving)\n",
      "342 8.529e-01 8.967e-01\n",
      "343 8.511e-01 9.095e-01\n",
      "344 8.480e-01 9.210e-01\n",
      "345 8.417e-01 9.047e-01\n",
      "346 8.470e-01 9.243e-01\n",
      "347 8.694e-01 9.096e-01\n",
      "348 8.427e-01 9.190e-01\n",
      "349 8.357e-01 9.267e-01\n",
      "350 8.443e-01 9.078e-01\n",
      "351 8.478e-01 9.156e-01\n",
      "352 8.395e-01 9.178e-01\n",
      "353 8.472e-01 9.111e-01\n",
      "354 8.481e-01 9.118e-01\n",
      "355 8.389e-01 9.089e-01\n",
      "356 8.481e-01 9.002e-01\n",
      "357 8.385e-01 9.189e-01\n",
      "358 8.421e-01 8.875e-01 (saving)\n",
      "359 8.330e-01 9.013e-01\n",
      "360 8.469e-01 9.116e-01\n",
      "361 8.333e-01 9.011e-01\n",
      "362 8.414e-01 9.069e-01\n",
      "363 8.427e-01 9.078e-01\n",
      "364 8.476e-01 9.090e-01\n",
      "365 8.178e-01 9.319e-01\n",
      "366 8.499e-01 9.147e-01\n",
      "367 8.460e-01 9.240e-01\n",
      "368 8.461e-01 9.133e-01\n",
      "369 8.425e-01 9.014e-01\n",
      "370 8.434e-01 9.185e-01\n",
      "371 8.408e-01 9.092e-01\n",
      "372 8.389e-01 9.098e-01\n",
      "373 8.262e-01 9.041e-01\n",
      "374 8.521e-01 9.053e-01\n",
      "375 8.375e-01 9.053e-01\n",
      "376 8.494e-01 9.185e-01\n",
      "377 8.475e-01 9.060e-01\n",
      "378 8.451e-01 9.001e-01\n",
      "379 8.317e-01 9.030e-01\n",
      "380 8.426e-01 9.017e-01\n",
      "381 8.469e-01 9.287e-01\n",
      "382 8.377e-01 9.175e-01\n",
      "383 8.364e-01 9.111e-01\n",
      "384 8.486e-01 9.271e-01\n",
      "385 8.466e-01 9.349e-01\n",
      "386 8.437e-01 9.257e-01\n",
      "387 8.584e-01 9.102e-01\n",
      "388 8.347e-01 9.024e-01\n",
      "389 8.443e-01 9.309e-01\n",
      "390 8.391e-01 9.180e-01\n",
      "391 8.248e-01 9.170e-01\n",
      "392 8.529e-01 8.953e-01\n",
      "393 8.397e-01 9.645e-01\n",
      "394 8.561e-01 9.228e-01\n",
      "395 8.408e-01 9.191e-01\n",
      "396 8.389e-01 8.963e-01\n",
      "397 8.476e-01 8.942e-01\n",
      "398 8.440e-01 9.231e-01\n",
      "399 8.420e-01 9.044e-01\n",
      "400 8.381e-01 9.275e-01\n",
      "401 8.297e-01 8.996e-01\n",
      "402 8.376e-01 9.100e-01\n",
      "403 8.358e-01 9.232e-01\n",
      "404 8.444e-01 9.240e-01\n",
      "405 8.301e-01 9.238e-01\n",
      "406 8.347e-01 9.218e-01\n",
      "407 8.348e-01 9.214e-01\n",
      "408 8.375e-01 9.619e-01\n",
      "409 8.438e-01 8.961e-01\n",
      "410 8.422e-01 9.089e-01\n",
      "411 8.514e-01 9.287e-01\n",
      "412 8.493e-01 9.128e-01\n",
      "413 8.458e-01 9.116e-01\n",
      "414 8.524e-01 9.134e-01\n",
      "415 8.448e-01 9.107e-01\n",
      "416 8.444e-01 9.071e-01\n",
      "417 8.349e-01 9.111e-01\n",
      "418 8.161e-01 9.290e-01\n",
      "419 8.369e-01 8.993e-01\n",
      "420 8.318e-01 9.057e-01\n",
      "421 8.263e-01 9.370e-01\n",
      "422 8.383e-01 9.124e-01\n",
      "423 8.325e-01 9.071e-01\n",
      "424 8.267e-01 9.106e-01\n",
      "425 8.501e-01 9.692e-01\n",
      "426 8.553e-01 9.079e-01\n",
      "427 8.338e-01 9.190e-01\n",
      "428 8.478e-01 9.301e-01\n",
      "429 8.336e-01 9.286e-01\n",
      "430 8.340e-01 9.084e-01\n",
      "431 8.294e-01 9.067e-01\n",
      "432 8.426e-01 9.123e-01\n",
      "433 8.425e-01 9.088e-01\n",
      "434 8.394e-01 9.158e-01\n",
      "435 8.395e-01 9.164e-01\n",
      "436 8.320e-01 9.102e-01\n",
      "437 8.294e-01 9.044e-01\n",
      "438 8.281e-01 9.024e-01\n",
      "439 8.444e-01 9.113e-01\n",
      "440 8.257e-01 9.140e-01\n",
      "441 8.447e-01 9.260e-01\n",
      "442 8.514e-01 9.100e-01\n",
      "443 8.484e-01 9.113e-01\n",
      "444 8.193e-01 9.149e-01\n",
      "445 8.295e-01 9.207e-01\n",
      "446 8.336e-01 9.298e-01\n",
      "447 8.449e-01 9.156e-01\n",
      "448 8.382e-01 9.142e-01\n",
      "449 8.220e-01 9.242e-01\n",
      "450 8.440e-01 9.095e-01\n",
      "451 8.345e-01 9.217e-01\n",
      "452 8.324e-01 9.165e-01\n",
      "453 8.510e-01 9.230e-01\n",
      "454 8.220e-01 9.155e-01\n",
      "455 8.278e-01 9.214e-01\n",
      "456 8.302e-01 9.159e-01\n",
      "457 8.314e-01 9.050e-01\n",
      "458 8.239e-01 9.006e-01\n",
      "459 8.334e-01 9.237e-01\n",
      "460 8.385e-01 9.280e-01\n",
      "461 8.298e-01 9.180e-01\n",
      "462 8.323e-01 9.092e-01\n",
      "463 8.344e-01 9.297e-01\n",
      "464 8.371e-01 9.040e-01\n",
      "465 8.461e-01 9.226e-01\n",
      "466 8.458e-01 9.624e-01\n",
      "467 8.333e-01 9.226e-01\n",
      "468 8.392e-01 9.150e-01\n",
      "469 8.291e-01 9.233e-01\n",
      "470 8.378e-01 9.290e-01\n",
      "471 8.325e-01 9.166e-01\n",
      "472 8.252e-01 9.237e-01\n",
      "473 8.222e-01 9.165e-01\n",
      "474 8.349e-01 9.005e-01\n",
      "475 8.395e-01 9.022e-01\n",
      "476 8.433e-01 9.125e-01\n",
      "477 8.346e-01 9.113e-01\n",
      "478 8.301e-01 9.388e-01\n",
      "479 8.307e-01 9.187e-01\n",
      "480 8.337e-01 9.014e-01\n",
      "481 8.227e-01 9.135e-01\n",
      "482 8.413e-01 9.386e-01\n",
      "483 8.458e-01 9.596e-01\n",
      "484 8.276e-01 9.259e-01\n",
      "485 8.237e-01 9.380e-01\n",
      "486 8.266e-01 9.125e-01\n",
      "487 8.287e-01 9.299e-01\n",
      "488 8.392e-01 9.130e-01\n",
      "489 8.279e-01 9.289e-01\n",
      "490 8.264e-01 9.102e-01\n",
      "491 8.169e-01 9.391e-01\n",
      "492 8.171e-01 9.049e-01\n",
      "493 8.238e-01 9.319e-01\n",
      "494 8.375e-01 9.282e-01\n",
      "495 8.298e-01 9.196e-01\n",
      "496 8.261e-01 9.420e-01\n",
      "497 8.305e-01 9.268e-01\n",
      "498 8.315e-01 9.397e-01\n",
      "499 8.133e-01 9.229e-01\n",
      "500 8.316e-01 9.403e-01\n",
      "501 8.241e-01 9.017e-01\n",
      "502 8.411e-01 9.347e-01\n",
      "503 8.265e-01 9.023e-01\n",
      "504 8.195e-01 9.442e-01\n",
      "505 8.356e-01 9.102e-01\n",
      "506 8.207e-01 9.057e-01\n",
      "507 8.177e-01 9.235e-01\n",
      "508 8.233e-01 9.138e-01\n",
      "509 8.256e-01 9.061e-01\n",
      "510 8.314e-01 9.045e-01\n",
      "511 8.334e-01 9.162e-01\n",
      "512 8.118e-01 8.964e-01\n",
      "513 8.340e-01 9.505e-01\n",
      "514 8.273e-01 9.479e-01\n",
      "515 8.265e-01 9.290e-01\n",
      "516 8.359e-01 9.077e-01\n",
      "517 8.186e-01 9.119e-01\n",
      "518 8.178e-01 9.280e-01\n",
      "519 8.352e-01 9.127e-01\n",
      "520 8.305e-01 9.137e-01\n",
      "521 8.208e-01 9.062e-01\n",
      "522 8.252e-01 9.009e-01\n",
      "523 8.300e-01 9.167e-01\n",
      "524 8.272e-01 9.039e-01\n",
      "525 8.237e-01 9.143e-01\n",
      "526 8.204e-01 9.212e-01\n",
      "527 8.344e-01 9.359e-01\n",
      "528 8.531e-01 9.198e-01\n",
      "529 8.283e-01 9.405e-01\n",
      "530 8.368e-01 9.036e-01\n",
      "531 8.198e-01 9.169e-01\n",
      "532 8.199e-01 9.210e-01\n",
      "533 8.044e-01 9.367e-01\n",
      "534 8.100e-01 9.238e-01\n",
      "535 8.178e-01 9.373e-01\n",
      "536 8.241e-01 9.107e-01\n",
      "537 8.178e-01 9.237e-01\n",
      "538 8.268e-01 9.122e-01\n",
      "539 8.352e-01 9.078e-01\n",
      "540 8.451e-01 9.195e-01\n",
      "541 8.262e-01 9.110e-01\n",
      "542 8.291e-01 9.275e-01\n",
      "543 8.216e-01 9.206e-01\n",
      "544 8.303e-01 9.014e-01\n",
      "545 8.260e-01 9.110e-01\n",
      "546 8.242e-01 9.115e-01\n",
      "547 8.325e-01 9.201e-01\n",
      "548 8.187e-01 9.312e-01\n",
      "549 8.284e-01 9.152e-01\n",
      "550 8.291e-01 9.173e-01\n",
      "551 8.188e-01 9.092e-01\n",
      "552 8.376e-01 9.329e-01\n",
      "553 8.108e-01 9.352e-01\n",
      "554 8.193e-01 9.204e-01\n",
      "555 8.269e-01 9.214e-01\n",
      "556 8.298e-01 9.050e-01\n",
      "557 8.086e-01 9.260e-01\n",
      "558 8.160e-01 9.324e-01\n",
      "559 8.190e-01 9.016e-01\n",
      "560 8.157e-01 9.695e-01\n",
      "561 8.174e-01 9.191e-01\n",
      "562 8.183e-01 9.196e-01\n",
      "563 8.108e-01 9.177e-01\n",
      "564 8.176e-01 9.207e-01\n",
      "565 8.282e-01 9.176e-01\n",
      "566 8.130e-01 9.224e-01\n",
      "567 8.240e-01 9.234e-01\n",
      "568 8.164e-01 9.320e-01\n",
      "569 8.083e-01 9.525e-01\n",
      "570 8.150e-01 9.110e-01\n",
      "571 8.156e-01 9.143e-01\n",
      "572 8.076e-01 9.331e-01\n",
      "573 8.371e-01 9.208e-01\n",
      "574 8.119e-01 9.172e-01\n",
      "575 8.198e-01 9.127e-01\n",
      "576 8.340e-01 9.116e-01\n",
      "577 8.223e-01 9.442e-01\n",
      "578 8.224e-01 9.137e-01\n",
      "579 8.410e-01 9.228e-01\n",
      "580 8.188e-01 9.148e-01\n",
      "581 8.262e-01 9.390e-01\n",
      "582 8.178e-01 9.214e-01\n",
      "583 8.333e-01 9.300e-01\n",
      "584 8.256e-01 9.177e-01\n",
      "585 8.128e-01 9.051e-01\n",
      "586 8.101e-01 8.935e-01\n",
      "587 8.276e-01 9.055e-01\n",
      "588 8.118e-01 9.514e-01\n",
      "589 8.082e-01 9.179e-01\n",
      "590 8.187e-01 9.095e-01\n",
      "591 8.245e-01 9.169e-01\n",
      "592 8.122e-01 9.355e-01\n",
      "593 8.184e-01 9.093e-01\n",
      "594 8.038e-01 9.262e-01\n",
      "595 8.120e-01 9.113e-01\n",
      "596 8.273e-01 9.116e-01\n",
      "597 8.184e-01 9.255e-01\n",
      "598 8.047e-01 9.112e-01\n",
      "599 8.186e-01 9.181e-01\n",
      "600 8.218e-01 9.432e-01\n",
      "601 8.072e-01 9.213e-01\n",
      "602 8.165e-01 9.215e-01\n",
      "603 8.327e-01 9.237e-01\n",
      "604 8.158e-01 9.246e-01\n",
      "605 8.223e-01 9.228e-01\n",
      "606 8.114e-01 9.045e-01\n",
      "607 8.256e-01 9.016e-01\n",
      "608 8.026e-01 9.301e-01\n",
      "609 8.081e-01 9.030e-01\n",
      "610 8.220e-01 9.358e-01\n",
      "611 8.248e-01 9.238e-01\n",
      "612 8.099e-01 9.303e-01\n",
      "613 8.012e-01 9.223e-01\n",
      "614 8.164e-01 9.363e-01\n",
      "615 8.331e-01 9.344e-01\n",
      "616 8.240e-01 9.092e-01\n",
      "617 8.314e-01 9.347e-01\n",
      "618 8.176e-01 9.390e-01\n",
      "619 8.221e-01 9.419e-01\n",
      "620 8.200e-01 9.090e-01\n",
      "621 8.149e-01 9.274e-01\n",
      "622 8.200e-01 9.209e-01\n",
      "623 8.269e-01 9.124e-01\n",
      "624 8.001e-01 9.217e-01\n",
      "625 8.097e-01 9.162e-01\n",
      "626 8.062e-01 9.130e-01\n",
      "627 8.111e-01 9.583e-01\n",
      "628 8.095e-01 9.110e-01\n",
      "629 8.089e-01 9.136e-01\n",
      "630 8.047e-01 9.207e-01\n",
      "631 8.190e-01 9.087e-01\n",
      "632 8.106e-01 9.200e-01\n",
      "633 8.121e-01 9.292e-01\n",
      "634 8.130e-01 9.215e-01\n",
      "635 8.104e-01 9.124e-01\n",
      "636 8.290e-01 9.043e-01\n",
      "637 8.183e-01 9.415e-01\n",
      "638 8.013e-01 9.278e-01\n",
      "639 8.203e-01 9.285e-01\n",
      "640 8.164e-01 9.089e-01\n",
      "641 8.160e-01 9.336e-01\n",
      "642 8.068e-01 9.372e-01\n",
      "643 8.163e-01 9.345e-01\n",
      "644 8.168e-01 9.192e-01\n",
      "645 8.133e-01 9.082e-01\n",
      "646 8.258e-01 9.251e-01\n",
      "647 8.133e-01 9.463e-01\n",
      "648 8.174e-01 9.056e-01\n",
      "649 7.860e-01 9.557e-01\n",
      "650 8.084e-01 9.030e-01\n",
      "651 8.194e-01 9.139e-01\n",
      "652 8.155e-01 9.419e-01\n",
      "653 8.178e-01 9.211e-01\n",
      "654 8.177e-01 9.137e-01\n",
      "655 7.993e-01 9.140e-01\n",
      "656 8.126e-01 9.210e-01\n",
      "657 8.235e-01 8.996e-01\n",
      "658 8.165e-01 9.157e-01\n",
      "659 8.211e-01 9.242e-01\n",
      "660 8.153e-01 9.281e-01\n",
      "661 8.129e-01 9.122e-01\n",
      "662 8.159e-01 9.184e-01\n",
      "663 8.065e-01 9.299e-01\n",
      "664 8.153e-01 9.129e-01\n",
      "665 7.987e-01 9.244e-01\n",
      "666 8.066e-01 9.157e-01\n",
      "667 8.086e-01 9.458e-01\n",
      "668 8.137e-01 9.408e-01\n",
      "669 8.087e-01 9.124e-01\n",
      "670 8.195e-01 9.410e-01\n",
      "671 8.021e-01 9.335e-01\n",
      "672 8.299e-01 9.242e-01\n",
      "673 7.949e-01 9.262e-01\n",
      "674 8.078e-01 9.467e-01\n",
      "675 8.219e-01 9.189e-01\n",
      "676 7.964e-01 9.337e-01\n",
      "677 8.265e-01 9.156e-01\n",
      "678 8.145e-01 9.212e-01\n",
      "679 8.313e-01 9.432e-01\n",
      "680 8.160e-01 9.224e-01\n",
      "681 8.170e-01 9.128e-01\n",
      "682 8.031e-01 9.137e-01\n",
      "683 8.001e-01 9.332e-01\n",
      "684 8.280e-01 9.029e-01\n",
      "685 8.116e-01 9.097e-01\n",
      "686 8.129e-01 9.321e-01\n",
      "687 8.170e-01 9.223e-01\n",
      "688 8.063e-01 9.201e-01\n",
      "689 8.013e-01 9.377e-01\n",
      "690 8.099e-01 9.082e-01\n",
      "691 8.110e-01 9.093e-01\n",
      "692 7.989e-01 9.278e-01\n",
      "693 8.108e-01 8.997e-01\n",
      "694 8.105e-01 9.251e-01\n",
      "695 8.127e-01 9.150e-01\n",
      "696 8.084e-01 9.138e-01\n",
      "697 7.943e-01 9.167e-01\n",
      "698 8.262e-01 9.002e-01\n",
      "699 8.139e-01 9.419e-01\n",
      "700 8.174e-01 9.249e-01\n",
      "701 8.075e-01 9.221e-01\n",
      "702 7.975e-01 9.283e-01\n",
      "703 8.085e-01 9.216e-01\n",
      "704 8.148e-01 9.102e-01\n",
      "705 8.101e-01 9.067e-01\n",
      "706 8.124e-01 9.095e-01\n",
      "707 8.020e-01 9.210e-01\n",
      "708 8.071e-01 9.329e-01\n",
      "709 7.996e-01 9.356e-01\n",
      "710 7.981e-01 9.156e-01\n",
      "711 8.129e-01 9.280e-01\n",
      "712 7.976e-01 9.453e-01\n",
      "713 8.099e-01 9.030e-01\n",
      "714 8.052e-01 9.124e-01\n",
      "715 7.972e-01 9.135e-01\n",
      "716 8.079e-01 9.044e-01\n",
      "717 8.240e-01 9.138e-01\n",
      "718 8.176e-01 9.155e-01\n",
      "719 8.238e-01 9.062e-01\n",
      "720 8.108e-01 9.012e-01\n",
      "721 8.067e-01 9.177e-01\n",
      "722 7.961e-01 9.023e-01\n",
      "723 8.122e-01 9.091e-01\n",
      "724 8.175e-01 9.228e-01\n",
      "725 7.964e-01 9.449e-01\n",
      "726 8.102e-01 9.053e-01\n",
      "727 8.114e-01 9.114e-01\n",
      "728 8.061e-01 9.039e-01\n",
      "729 8.118e-01 9.055e-01\n",
      "730 8.008e-01 9.097e-01\n",
      "731 8.029e-01 9.181e-01\n",
      "732 8.064e-01 9.070e-01\n",
      "733 7.953e-01 9.203e-01\n",
      "734 8.066e-01 9.237e-01\n",
      "735 7.989e-01 9.120e-01\n",
      "736 8.083e-01 9.260e-01\n",
      "737 7.990e-01 9.452e-01\n",
      "738 8.056e-01 9.482e-01\n",
      "739 7.855e-01 9.216e-01\n",
      "740 8.031e-01 8.972e-01\n",
      "741 7.961e-01 9.208e-01\n",
      "742 8.083e-01 9.650e-01\n",
      "743 8.104e-01 9.389e-01\n",
      "744 8.051e-01 9.355e-01\n",
      "745 8.043e-01 9.152e-01\n",
      "746 7.884e-01 9.279e-01\n",
      "747 8.220e-01 9.060e-01\n",
      "748 7.921e-01 9.091e-01\n",
      "749 7.937e-01 9.112e-01\n",
      "750 8.049e-01 9.183e-01\n",
      "751 8.059e-01 9.014e-01\n",
      "752 8.019e-01 9.343e-01\n",
      "753 8.192e-01 9.212e-01\n",
      "754 8.113e-01 9.233e-01\n",
      "755 7.986e-01 9.165e-01\n",
      "756 7.843e-01 9.146e-01\n",
      "757 8.169e-01 9.509e-01\n",
      "758 8.004e-01 9.133e-01\n",
      "759 7.897e-01 9.033e-01\n",
      "760 8.154e-01 9.089e-01\n",
      "761 8.215e-01 9.135e-01\n",
      "762 8.105e-01 9.293e-01\n",
      "763 8.090e-01 9.471e-01\n",
      "764 8.134e-01 9.740e-01\n",
      "765 8.151e-01 9.069e-01\n",
      "766 7.960e-01 9.422e-01\n",
      "767 8.041e-01 9.242e-01\n",
      "768 8.119e-01 9.274e-01\n",
      "769 8.131e-01 9.269e-01\n",
      "770 8.102e-01 9.163e-01\n",
      "771 8.104e-01 9.196e-01\n",
      "772 8.213e-01 9.217e-01\n",
      "773 8.166e-01 9.224e-01\n",
      "774 7.987e-01 9.351e-01\n",
      "775 8.265e-01 9.558e-01\n",
      "776 7.847e-01 9.369e-01\n",
      "777 7.950e-01 9.283e-01\n",
      "778 8.119e-01 9.404e-01\n",
      "779 8.143e-01 9.139e-01\n",
      "780 8.122e-01 8.857e-01 (saving)\n",
      "781 8.061e-01 9.318e-01\n",
      "782 7.949e-01 9.137e-01\n",
      "783 7.967e-01 9.154e-01\n",
      "784 7.993e-01 9.223e-01\n",
      "785 8.049e-01 9.186e-01\n",
      "786 8.107e-01 9.355e-01\n",
      "787 7.838e-01 9.380e-01\n",
      "788 8.079e-01 9.203e-01\n",
      "789 8.075e-01 8.961e-01\n",
      "790 8.120e-01 9.355e-01\n",
      "791 8.078e-01 9.189e-01\n",
      "792 7.950e-01 9.341e-01\n",
      "793 8.184e-01 9.222e-01\n",
      "794 7.970e-01 9.062e-01\n",
      "795 7.938e-01 9.064e-01\n",
      "796 7.951e-01 9.264e-01\n",
      "797 8.102e-01 9.136e-01\n",
      "798 7.925e-01 9.332e-01\n",
      "799 7.901e-01 9.129e-01\n",
      "800 8.087e-01 9.161e-01\n",
      "801 7.890e-01 9.861e-01\n",
      "802 8.105e-01 9.336e-01\n",
      "803 8.081e-01 9.355e-01\n",
      "804 8.094e-01 9.277e-01\n",
      "805 8.014e-01 9.441e-01\n",
      "806 7.979e-01 9.173e-01\n",
      "807 8.058e-01 9.112e-01\n",
      "808 8.043e-01 9.154e-01\n",
      "809 8.068e-01 9.526e-01\n",
      "810 8.074e-01 9.340e-01\n",
      "811 8.034e-01 9.169e-01\n",
      "812 8.156e-01 9.017e-01\n",
      "813 8.011e-01 9.194e-01\n",
      "814 8.128e-01 9.413e-01\n",
      "815 7.936e-01 9.391e-01\n",
      "816 7.982e-01 9.171e-01\n",
      "817 7.955e-01 9.328e-01\n",
      "818 8.044e-01 9.088e-01\n",
      "819 8.161e-01 9.084e-01\n",
      "820 7.873e-01 9.283e-01\n",
      "821 8.101e-01 9.604e-01\n",
      "822 8.033e-01 9.274e-01\n",
      "823 8.124e-01 9.303e-01\n",
      "824 8.122e-01 9.173e-01\n",
      "825 8.096e-01 9.655e-01\n",
      "826 8.127e-01 9.109e-01\n",
      "827 8.038e-01 9.342e-01\n",
      "828 7.821e-01 9.373e-01\n",
      "829 8.058e-01 9.075e-01\n",
      "830 7.974e-01 9.219e-01\n",
      "831 8.013e-01 9.682e-01\n",
      "832 8.077e-01 9.071e-01\n",
      "833 8.016e-01 9.607e-01\n",
      "834 8.085e-01 9.226e-01\n",
      "835 8.041e-01 9.288e-01\n",
      "836 7.945e-01 9.172e-01\n",
      "837 8.006e-01 9.089e-01\n",
      "838 7.992e-01 8.930e-01\n",
      "839 8.067e-01 9.341e-01\n",
      "840 8.027e-01 8.984e-01\n",
      "841 8.013e-01 9.339e-01\n",
      "842 7.994e-01 9.336e-01\n",
      "843 7.982e-01 9.359e-01\n",
      "844 8.087e-01 9.666e-01\n",
      "845 7.960e-01 9.286e-01\n",
      "846 8.123e-01 9.119e-01\n",
      "847 8.014e-01 9.326e-01\n",
      "848 7.998e-01 9.459e-01\n",
      "849 7.926e-01 9.238e-01\n",
      "850 7.931e-01 9.382e-01\n",
      "851 7.938e-01 9.515e-01\n",
      "852 8.019e-01 9.190e-01\n",
      "853 8.076e-01 9.263e-01\n",
      "854 8.009e-01 9.146e-01\n",
      "855 7.989e-01 9.323e-01\n",
      "856 7.972e-01 9.290e-01\n",
      "857 8.017e-01 9.324e-01\n",
      "858 8.062e-01 9.402e-01\n",
      "859 7.926e-01 9.396e-01\n",
      "860 7.806e-01 9.456e-01\n",
      "861 8.001e-01 9.347e-01\n",
      "862 7.952e-01 9.261e-01\n",
      "863 7.998e-01 9.141e-01\n",
      "864 7.983e-01 9.144e-01\n",
      "865 7.848e-01 9.229e-01\n",
      "866 8.000e-01 9.108e-01\n",
      "867 7.973e-01 9.039e-01\n",
      "868 8.062e-01 9.296e-01\n",
      "869 7.928e-01 9.308e-01\n",
      "870 8.002e-01 9.327e-01\n",
      "871 8.142e-01 9.283e-01\n",
      "872 8.097e-01 9.116e-01\n",
      "873 8.016e-01 9.097e-01\n",
      "874 8.066e-01 9.181e-01\n",
      "875 8.085e-01 9.154e-01\n",
      "876 8.142e-01 9.254e-01\n",
      "877 8.048e-01 9.178e-01\n",
      "878 8.136e-01 9.327e-01\n",
      "879 8.035e-01 9.408e-01\n",
      "880 8.002e-01 9.281e-01\n",
      "881 8.068e-01 9.339e-01\n",
      "882 8.086e-01 9.458e-01\n",
      "883 8.033e-01 9.234e-01\n",
      "884 7.813e-01 9.037e-01\n",
      "885 7.999e-01 9.150e-01\n",
      "886 7.940e-01 9.356e-01\n",
      "887 8.001e-01 9.625e-01\n",
      "888 7.902e-01 9.221e-01\n",
      "889 7.921e-01 9.284e-01\n",
      "890 8.137e-01 9.128e-01\n",
      "891 8.000e-01 9.366e-01\n",
      "892 7.882e-01 9.383e-01\n",
      "893 8.058e-01 9.215e-01\n",
      "894 8.075e-01 9.550e-01\n",
      "895 7.991e-01 9.290e-01\n",
      "896 7.909e-01 9.316e-01\n",
      "897 8.108e-01 9.490e-01\n",
      "898 8.042e-01 9.058e-01\n",
      "899 8.104e-01 9.166e-01\n",
      "900 8.170e-01 9.253e-01\n",
      "901 8.099e-01 9.081e-01\n",
      "902 7.982e-01 9.084e-01\n",
      "903 7.959e-01 9.214e-01\n",
      "904 8.032e-01 9.233e-01\n",
      "905 8.075e-01 9.339e-01\n",
      "906 8.082e-01 9.136e-01\n",
      "907 8.053e-01 9.207e-01\n",
      "908 7.744e-01 9.284e-01\n",
      "909 8.037e-01 9.348e-01\n",
      "910 8.047e-01 9.231e-01\n",
      "911 7.882e-01 9.093e-01\n",
      "912 7.991e-01 9.188e-01\n",
      "913 8.116e-01 9.168e-01\n",
      "914 8.104e-01 9.103e-01\n",
      "915 7.761e-01 9.126e-01\n",
      "916 7.958e-01 9.426e-01\n",
      "917 8.006e-01 9.050e-01\n",
      "918 8.116e-01 9.132e-01\n",
      "919 7.981e-01 9.066e-01\n",
      "920 7.985e-01 9.218e-01\n",
      "921 8.009e-01 9.199e-01\n",
      "922 7.918e-01 9.129e-01\n",
      "923 7.945e-01 9.085e-01\n",
      "924 7.823e-01 9.212e-01\n",
      "925 7.750e-01 9.141e-01\n",
      "926 8.048e-01 9.282e-01\n",
      "927 8.222e-01 9.223e-01\n",
      "928 8.019e-01 9.405e-01\n",
      "929 8.164e-01 9.478e-01\n",
      "930 7.889e-01 9.185e-01\n",
      "931 7.985e-01 9.046e-01\n",
      "932 7.847e-01 9.408e-01\n",
      "933 7.983e-01 9.187e-01\n",
      "934 7.979e-01 9.220e-01\n",
      "935 8.020e-01 9.224e-01\n",
      "936 7.867e-01 9.180e-01\n",
      "937 8.053e-01 9.157e-01\n",
      "938 7.952e-01 9.100e-01\n",
      "939 8.077e-01 9.381e-01\n",
      "940 8.042e-01 9.062e-01\n",
      "941 7.961e-01 9.064e-01\n",
      "942 7.974e-01 9.438e-01\n",
      "943 7.888e-01 9.348e-01\n",
      "944 7.851e-01 9.140e-01\n",
      "945 8.056e-01 9.267e-01\n",
      "946 8.102e-01 9.254e-01\n",
      "947 7.873e-01 9.122e-01\n",
      "948 7.848e-01 9.264e-01\n",
      "949 7.805e-01 9.287e-01\n",
      "950 7.869e-01 9.129e-01\n",
      "951 8.123e-01 9.149e-01\n",
      "952 7.936e-01 9.275e-01\n",
      "953 7.778e-01 9.155e-01\n",
      "954 7.985e-01 9.191e-01\n",
      "955 7.941e-01 8.971e-01\n",
      "956 7.998e-01 9.151e-01\n",
      "957 7.803e-01 9.061e-01\n",
      "958 7.906e-01 9.231e-01\n",
      "959 7.908e-01 9.214e-01\n",
      "960 7.875e-01 9.693e-01\n",
      "961 8.173e-01 9.210e-01\n",
      "962 7.861e-01 9.246e-01\n",
      "963 8.020e-01 9.213e-01\n",
      "964 8.033e-01 9.091e-01\n",
      "965 7.797e-01 9.173e-01\n",
      "966 7.898e-01 9.091e-01\n",
      "967 7.950e-01 9.109e-01\n",
      "968 7.868e-01 9.038e-01\n",
      "969 7.894e-01 9.360e-01\n",
      "970 8.010e-01 9.088e-01\n",
      "971 7.865e-01 9.279e-01\n",
      "972 7.984e-01 9.144e-01\n",
      "973 7.991e-01 9.246e-01\n",
      "974 8.002e-01 9.273e-01\n",
      "975 8.100e-01 9.247e-01\n",
      "976 7.886e-01 9.246e-01\n",
      "977 7.950e-01 9.078e-01\n",
      "978 7.814e-01 9.517e-01\n",
      "979 8.007e-01 9.214e-01\n",
      "980 7.917e-01 9.161e-01\n",
      "981 7.999e-01 9.260e-01\n",
      "982 8.087e-01 9.282e-01\n",
      "983 8.030e-01 9.216e-01\n",
      "984 7.797e-01 9.230e-01\n",
      "985 8.030e-01 9.276e-01\n",
      "986 8.089e-01 9.107e-01\n",
      "987 7.941e-01 9.273e-01\n",
      "988 8.077e-01 9.116e-01\n",
      "989 8.015e-01 9.018e-01\n",
      "990 8.019e-01 9.144e-01\n",
      "991 8.005e-01 9.261e-01\n",
      "992 7.999e-01 9.079e-01\n",
      "993 7.907e-01 9.283e-01\n",
      "994 8.086e-01 9.176e-01\n",
      "995 8.030e-01 9.031e-01\n",
      "996 7.870e-01 9.384e-01\n",
      "997 8.156e-01 9.234e-01\n",
      "998 7.903e-01 9.037e-01\n",
      "999 8.144e-01 9.110e-01\n",
      "Time take (m): 3.6384\n"
     ]
    }
   ],
   "source": [
    "fout   = 'test_new_var_opt.txt'\n",
    "fmodel = 'test_new_var_opt.pt'\n",
    "#casts parameters/buffers to specified gpu\n",
    "model.to(device=device)\n",
    "#get num elements in each network parameters and sum them\n",
    "network_total_params = sum(p.numel() for p in model.parameters())\n",
    "print('total number of parameters in the model = %d'%network_total_params)\n",
    "\n",
    "# define optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr_1, betas=(0.5, 0.999), \n",
    "                             weight_decay=wd_1)\t\n",
    "\n",
    "\n",
    "# load best-model, if it exists\n",
    "'''\n",
    "if os.path.exists(fmodel):  \n",
    "    print('Loading model...')\n",
    "    model.load_state_dict(torch.load(fmodel))\n",
    "# get validation loss\n",
    "print('Computing initial validation loss')\n",
    "model.eval()\n",
    "min_valid_loss, points = 0.0, 0\n",
    "for x, y in valid_loader:\n",
    "    with torch.no_grad():\n",
    "        x    = x.to(device=device)\n",
    "        y    = y.to(device=device)\n",
    "        y_NN = model(x)\n",
    "        min_valid_loss += (criterion(y_NN, y).item())*x.shape[0]\n",
    "        points += x.shape[0]\n",
    "min_valid_loss /= points\n",
    "print('Initial valid loss = %.3e'%min_valid_loss)\n",
    "'''\n",
    "# see if results for this model are available (if you're continuing training on a model)\n",
    "'''\n",
    "if os.path.exists(fout):  \n",
    "    dumb = np.loadtxt(fout, skiprows = 11, unpack=False)\n",
    "    if dumb.size == 0:\n",
    "        offset = 0\n",
    "    else: offset = int(dumb[:,0][-1]+1)\n",
    "else:   offset = 0\n",
    "'''\n",
    "\n",
    "# do a loop over all epochs\n",
    "start = time.time()\n",
    "min_valid_loss = 1000\n",
    "for epoch in range(epochs):\n",
    "    # do training\n",
    "    train_loss, points = 0.0, 0\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_NN = model(x)\n",
    "        #y = nn.Softmax(y)\n",
    "        #y_NN = nn.Softmax(y_NN)\n",
    "        loss = criterion(y_NN, y)\n",
    "        train_loss += (loss.item())*x.shape[0]\n",
    "        points     += x.shape[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= points\n",
    "\n",
    "    # do testing\n",
    "    test_loss, points = 0.0, 0\n",
    "    model.eval()\n",
    "    for x, y in test_loader:\n",
    "        with torch.no_grad():\n",
    "            x    = x.to(device)\n",
    "            y    = y.to(device)\n",
    "            #y = nn.Softmax(y)\n",
    "            y_NN = model(x)\n",
    "            #y_NN = nn.Softmax(y_NN)\n",
    "            test_loss += (criterion(y_NN, y).item())*x.shape[0]\n",
    "            points    += x.shape[0]\n",
    "    test_loss /= points\n",
    "    \n",
    "    # save model if it is better\n",
    "    if test_loss<min_valid_loss:\n",
    "        torch.save(model.state_dict(), fmodel)\n",
    "        min_valid_loss = test_loss\n",
    "        print('%03d %.3e %.3e (saving)'%(epoch, train_loss, test_loss))\n",
    "    else:\n",
    "        print('%03d %.3e %.3e'%(epoch, train_loss, test_loss))\n",
    "    \n",
    "    # save losses to file\n",
    "    f = open(fout, 'a')\n",
    "    f.write('%d %.5e %.5e\\n'%(epoch, train_loss, test_loss))\n",
    "    f.close()\n",
    "    \n",
    "stop = time.time()\n",
    "print('Time take (m):', \"{:.4f}\".format((stop-start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4f6fb88a-f1f5-4268-bb6c-0396bf99b26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# do validation\\nvalid_loss, points = 0.0, 0\\nmodel.eval()\\nfor x, y in valid_loader:\\n    with torch.no_grad():\\n        x    = x.to(device)\\n        y    = y.to(device)\\n        #y = nn.Softmax(y)\\n        y_NN = model(x)\\n        #y_NN = nn.Softmax(y_NN)\\n        valid_loss += (criterion(y_NN, y).item())*x.shape[0]\\n        points     += x.shape[0]\\nvalid_loss /= points\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# do validation\n",
    "valid_loss, points = 0.0, 0\n",
    "model.eval()\n",
    "for x, y in valid_loader:\n",
    "    with torch.no_grad():\n",
    "        x    = x.to(device)\n",
    "        y    = y.to(device)\n",
    "        #y = nn.Softmax(y)\n",
    "        y_NN = model(x)\n",
    "        #y_NN = nn.Softmax(y_NN)\n",
    "        valid_loss += (criterion(y_NN, y).item())*x.shape[0]\n",
    "        points     += x.shape[0]\n",
    "valid_loss /= points\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1089e4c5-0a0e-4315-9039-39a4bec2b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "losses = np.loadtxt('test_new_var_opt.txt')\n",
    "train_losses = losses[:,1]\n",
    "test_losses = losses[:,2]\n",
    "\n",
    "# losses_1 = np.loadtxt('test_optuna1.txt', delimiter = ' ')\n",
    "# train_losses_1 = losses_1[:,1]\n",
    "# test_losses_1 = losses_1[:, 2]\n",
    "\n",
    "# losses_2 = np.loadtxt('test_optuna2.txt', delimiter = ' ')\n",
    "# train_losses_2 = losses_2[:,1]\n",
    "# test_losses_2 = losses_2[:, 2]\n",
    "\n",
    "# losses_3 = np.loadtxt('test_optuna3.txt', delimiter = ' ')\n",
    "# train_losses_3 = losses_3[:,1]\n",
    "# test_losses_3 = losses_3[:, 2]\n",
    "\n",
    "#losses_non = np.loadtxt('test_non.txt', delimiter = ' ')\n",
    "#train_losses_non = losses_non[:,1]\n",
    "#test_losses_non = losses_non[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ec832dc-a995-4d65-9681-ee986bb41101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bsherwin51/Renaissance/Normal\n"
     ]
    }
   ],
   "source": [
    "! pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a652a5f9-cc47-4096-8924-d17d966d8df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label = 'train')\n",
    "plt.plot(test_losses, label = 'test')\n",
    "# plt.plot(train_losses_1, label = 'train Optuna 1')\n",
    "# plt.plot(test_losses_1, label = 'test Optuna 1')\n",
    "# plt.plot(train_losses_2, label = 'train Optuna 2')\n",
    "# plt.plot(test_losses_2, label = 'test Optuna 2')\n",
    "# plt.plot(train_losses_3, label = 'train Optuna 3')\n",
    "# plt.plot(test_losses_3, label = 'test Optuna 3')\n",
    "#plt.plot(train_losses_non, label = 'train no Optuna')\n",
    "# plt.plot(test_losses_non, label = 'test no Optuna')\n",
    "plt.xscale('log')\n",
    "plt.ylim(0,1.5)\n",
    "plt.ylim(0.7,1.4)\n",
    "plt.legend()\n",
    "plt.title(\"Normal Classification Model Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e5687-361d-49d5-a793-ae9b05aaff1e",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c648521a-80f0-4f0a-abd4-95726fcb99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_model(trial, input_size, output_size, max_layers=3, max_neurons_layers=500):\n",
    "    # define the tuple containing the different layers\n",
    "    layers = []\n",
    "    # get the number of hidden layers\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, max_layers)\n",
    "    # get the hidden layers\n",
    "    in_features = input_size\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, max_neurons_layers)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.8)\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features = out_features\n",
    "    # get the last layer\n",
    "    layers.append(nn.Linear(out_features, output_size))\n",
    "    # return the model\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5c1fd28-98bd-4a66-8454-9747e6a071f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f896bfc6-b2b3-4718-9385-da2419c377dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective(object):\n",
    "    def __init__(self, input_size, output_size, max_layers, max_neurons_layers, device,\n",
    "                 epochs, seed, batch_size):\n",
    "\n",
    "        self.input_size         = input_size\n",
    "        self.output_size        = output_size\n",
    "        self.max_layers         = max_layers\n",
    "        self.max_neurons_layers = max_neurons_layers\n",
    "        self.device             = device\n",
    "        self.epochs             = epochs\n",
    "        self.seed               = seed\n",
    "        self.batch_size         = batch_size\n",
    "\n",
    "    def __call__(self, trial):\n",
    "\n",
    "        # name of the files that will contain the losses and model weights\n",
    "        f1 = 'losses_%s'%prefix\n",
    "        f2 = 'models_%s'%prefix\n",
    "        if not(os.path.exists(f1)):  os.system('mkdir %s'%f1)\n",
    "        if not(os.path.exists(f2)):  os.system('mkdir %s'%f2)\n",
    "\n",
    "        fout   = 'losses_%s/loss_%d.txt'%(prefix, trial.number)\n",
    "        fmodel = 'models_%s/model_%d.pt'%(prefix, trial.number)\n",
    "        \n",
    "        dataname = 'fesc_test' # name of your dataset, for the loss file header\n",
    "        \n",
    "        # write properties & headers in case you need to refer in the future and you've changed the main.py file\n",
    "        f = open(fout, 'w')\n",
    "        f.write('training dataset: {}\\n'.format(dataname))\n",
    "        f.write('seed = {}\\numFeatures = {}\\nFeature List: {}\\nfluxSize = {}\\nbatch_size = {}\\nepochs     = {}\\n'.format(seed, input_size, features, output_size, batch_size, epochs))\n",
    "        f.write('0 epoch  1 train loss  2 test loss\\n')\n",
    "        f.close()\n",
    "\n",
    "        # generate the architecture\n",
    "        model = dynamic_model(trial, self.input_size, self.output_size, \n",
    "                            self.max_layers, self.max_neurons_layers).to(self.device)\n",
    "\n",
    "        # get the weight decay and learning rate values\n",
    "        ## adjust boundary values as needed\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "        wd = trial.suggest_float(\"wd\", 1e-8, 1e-2,  log=True)\n",
    "\n",
    "        # define the optimizer\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.5, 0.999), \n",
    "                                      weight_decay=wd)\n",
    "\n",
    "        # define loss function\n",
    "        criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "        # get the data\n",
    "        train_loader = create_dataset('train', self.seed, fin, self.batch_size, True)\n",
    "        valid_loader = create_dataset('valid', self.seed, fin, self.batch_size, False)\n",
    "        test_loader  = create_dataset('test',  self.seed, fin, self.batch_size, False)\n",
    "\n",
    "        # train/validate model\n",
    "        min_valid = 1e40\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # training\n",
    "            train_loss, points = 0.0, 0\n",
    "            model.train()\n",
    "            for x, y in train_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                y_NN = model(x)\n",
    "                \n",
    "                loss = criterion(y_NN, y)\n",
    "                train_loss += (loss.item())*x.shape[0]\n",
    "                points     += x.shape[0]\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss /= points\n",
    "            '''\n",
    "            # validation\n",
    "            valid_loss, points = 0.0, 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for x, y in valid_loader:\n",
    "                    x, y = x.to(self.device), y.to(self.device)\n",
    "                    y_NN = model(x)\n",
    "                    valid_loss += (criterion(y_NN, y).item())*x.shape[0]\n",
    "                    points     += x.shape[0]\n",
    "            valid_loss /= points\n",
    "            '''\n",
    "            # do testing\n",
    "            test_loss, points = 0.0, 0\n",
    "            model.eval()\n",
    "            for x, y in test_loader:\n",
    "                with torch.no_grad():\n",
    "                    x    = x.to(device)\n",
    "                    y    = y.to(device)\n",
    "                    y_NN = model(x)\n",
    "                    test_loss += (criterion(y_NN, y).item())*x.shape[0]\n",
    "                    points    += x.shape[0]\n",
    "            test_loss /= points\n",
    "            \n",
    "            #save this model if it's better\n",
    "            if test_loss<min_valid:  \n",
    "                min_valid = test_loss\n",
    "                torch.save(model.state_dict(), fmodel)\n",
    "            f = open(fout, 'a')\n",
    "            f.write('%d %.5e %.5e\\n'%(epoch, train_loss, test_loss))\n",
    "            f.close()\n",
    "\n",
    "            # Handle pruning based on the intermediate value\n",
    "            # comment out these lines if using pruning\n",
    "            #trial.report(min_valid, epoch)\n",
    "            #if trial.should_prune():  raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        return min_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7edcbc15-e51f-4181-b7f0-c113483fbccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### INPUT ##########################################\n",
    "# data parameters\n",
    "fin      = 'catalog.h5' # file containing your global properties\n",
    "features = 'SFRs, Mstars, masses, redshifts, fgases, min_dist, t_lookback' # writing the order of input data as per data.py. Modify as needed\n",
    "f_features_norm = None\n",
    "f_labels  = 'fesc_test_new_var.txt' # file containing output SEDs\n",
    "seed      = 5\n",
    "\n",
    "# optuna architecture parameters\n",
    "input_size         = 7 # number of input global properties\n",
    "output_size        = 4 # size of SED/wavelength array\n",
    "max_layers         = 5 # max number of hidden layers to test out\n",
    "max_neurons_layers = 500 # max number of nodes per hidden layer to test out\n",
    "\n",
    "# training parameters\n",
    "batch_size = 256\n",
    "epochs     = 1000\n",
    "\n",
    "# optuna parameters -- modify as needed\n",
    "prefix    = 'inp{}out{}maxl{}maxn{}bs{}ep{}'.format(input_size, output_size, max_layers, max_neurons_layers, batch_size, epochs)\n",
    "study_name       = 'fesc_new_var' \n",
    "n_trials         = 50 # set to None for infinite\n",
    "storage          = 'sqlite:///fesc_%s.db'%prefix \n",
    "n_jobs           = 1\n",
    "n_startup_trials = 30 # random sample the hyperparameter space before running the model sampler\n",
    "#########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "37f2b27e-672b-4649-ba09-97f42ddebd13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:08:30,022] A new study created in RDB with name: fesc_new_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:11:09,979] Trial 0 finished with value: 0.8488921589321561 and parameters: {'dropout_l0': 0.41098238393046577, 'dropout_l1': 0.7523606085988068, 'dropout_l2': 0.70925179733963, 'dropout_l3': 0.42101015173613254, 'lr': 0.0002813924856455179, 'n_layers': 4, 'n_units_l0': 307, 'n_units_l1': 336, 'n_units_l2': 431, 'n_units_l3': 359, 'wd': 2.244885870738801e-08}. Best is trial 0 with value: 0.8488921589321561.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:12:14,583] Trial 1 finished with value: 0.9296455434244922 and parameters: {'dropout_l0': 0.617632469627234, 'dropout_l1': 0.2267523293300885, 'lr': 5.7742693778631086e-05, 'n_layers': 2, 'n_units_l0': 81, 'n_units_l1': 54, 'wd': 0.0005834279965805242}. Best is trial 0 with value: 0.8488921589321561.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:13:33,815] Trial 2 finished with value: 0.8606635255691333 and parameters: {'dropout_l0': 0.4982045483608469, 'dropout_l1': 0.6204431309509015, 'lr': 0.00641293660358334, 'n_layers': 2, 'n_units_l0': 254, 'n_units_l1': 174, 'wd': 6.567652820124099e-07}. Best is trial 0 with value: 0.8488921589321561.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:14:58,471] Trial 3 finished with value: 0.8464392922882341 and parameters: {'dropout_l0': 0.5041965031728791, 'dropout_l1': 0.38671653165156267, 'dropout_l2': 0.34698216880268534, 'dropout_l3': 0.2756885912001197, 'lr': 0.006154313065575507, 'n_layers': 4, 'n_units_l0': 120, 'n_units_l1': 86, 'n_units_l2': 85, 'n_units_l3': 203, 'wd': 3.6609042448323692e-06}. Best is trial 3 with value: 0.8464392922882341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:17:29,401] Trial 4 finished with value: 0.854969797990261 and parameters: {'dropout_l0': 0.29366108903273963, 'dropout_l1': 0.4332826175019432, 'dropout_l2': 0.647220097028513, 'dropout_l3': 0.44865565046381495, 'dropout_l4': 0.7664533765482178, 'lr': 0.0001829096475517923, 'n_layers': 5, 'n_units_l0': 121, 'n_units_l1': 293, 'n_units_l2': 379, 'n_units_l3': 267, 'n_units_l4': 147, 'wd': 0.00024208097539936955}. Best is trial 3 with value: 0.8464392922882341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:19:35,662] Trial 5 finished with value: 0.8764463535740844 and parameters: {'dropout_l0': 0.4654421091192254, 'dropout_l1': 0.304698396197412, 'dropout_l2': 0.4823607618609073, 'dropout_l3': 0.6091984281065965, 'dropout_l4': 0.2129508283631626, 'lr': 0.005609968025545102, 'n_layers': 5, 'n_units_l0': 117, 'n_units_l1': 165, 'n_units_l2': 61, 'n_units_l3': 315, 'n_units_l4': 340, 'wd': 0.0018519322494184077}. Best is trial 3 with value: 0.8464392922882341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:20:42,753] Trial 6 finished with value: 0.860503249698215 and parameters: {'dropout_l0': 0.47896509529504566, 'lr': 0.0014717854147976981, 'n_layers': 1, 'n_units_l0': 393, 'wd': 0.007989818524075444}. Best is trial 3 with value: 0.8464392922882341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:21:53,363] Trial 7 finished with value: 0.8604627531817836 and parameters: {'dropout_l0': 0.3038512799370975, 'dropout_l1': 0.7834599787160712, 'lr': 0.006616716724935431, 'n_layers': 2, 'n_units_l0': 48, 'n_units_l1': 375, 'wd': 0.0022768193700500846}. Best is trial 3 with value: 0.8464392922882341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:23:20,129] Trial 8 finished with value: 0.8492434330475636 and parameters: {'dropout_l0': 0.7268935952752484, 'dropout_l1': 0.24015038111442122, 'lr': 0.0003546967314596553, 'n_layers': 2, 'n_units_l0': 425, 'n_units_l1': 197, 'wd': 1.448104920433874e-08}. Best is trial 3 with value: 0.8464392922882341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:24:28,478] Trial 9 finished with value: 0.870670896819514 and parameters: {'dropout_l0': 0.6307327577404103, 'dropout_l1': 0.43272267787125734, 'lr': 0.008189204828033621, 'n_layers': 2, 'n_units_l0': 35, 'n_units_l1': 363, 'wd': 2.215516184637607e-05}. Best is trial 3 with value: 0.8464392922882341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:26:24,983] Trial 10 finished with value: 0.8835695762919564 and parameters: {'dropout_l0': 0.6996850719506256, 'dropout_l1': 0.43915144128171213, 'dropout_l2': 0.7644528981846048, 'dropout_l3': 0.5712082961211451, 'dropout_l4': 0.21088754787284095, 'lr': 0.0031904785274712937, 'n_layers': 5, 'n_units_l0': 317, 'n_units_l1': 54, 'n_units_l2': 234, 'n_units_l3': 208, 'n_units_l4': 349, 'wd': 1.6792115895251684e-06}. Best is trial 3 with value: 0.8464392922882341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:28:10,602] Trial 11 finished with value: 0.9520516604439825 and parameters: {'dropout_l0': 0.36772765532740653, 'dropout_l1': 0.22181706658711775, 'dropout_l2': 0.7711633439888184, 'lr': 1.178253512787764e-05, 'n_layers': 3, 'n_units_l0': 463, 'n_units_l1': 242, 'n_units_l2': 38, 'wd': 0.00039829300734479066}. Best is trial 3 with value: 0.8464392922882341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:29:12,856] Trial 12 finished with value: 0.8486611736126435 and parameters: {'dropout_l0': 0.21283012857279382, 'lr': 0.0017902276909456318, 'n_layers': 1, 'n_units_l0': 245, 'wd': 3.0092743772811745e-06}. Best is trial 3 with value: 0.8464392922882341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:30:53,820] Trial 13 finished with value: 0.8397565647068187 and parameters: {'dropout_l0': 0.5881785123206221, 'dropout_l1': 0.7323844134623494, 'lr': 0.0010960356855057962, 'n_layers': 2, 'n_units_l0': 385, 'n_units_l1': 324, 'wd': 6.684685303081031e-05}. Best is trial 13 with value: 0.8397565647068187.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:32:27,456] Trial 14 finished with value: 0.9294811869278933 and parameters: {'dropout_l0': 0.3195056937997392, 'dropout_l1': 0.24041628786389985, 'dropout_l2': 0.2666832846399043, 'dropout_l3': 0.3665142526667983, 'dropout_l4': 0.4372707183719594, 'lr': 2.3118484542007762e-05, 'n_layers': 5, 'n_units_l0': 93, 'n_units_l1': 253, 'n_units_l2': 168, 'n_units_l3': 15, 'n_units_l4': 171, 'wd': 0.00016225622564974067}. Best is trial 13 with value: 0.8397565647068187.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:34:04,759] Trial 15 finished with value: 0.838355183091938 and parameters: {'dropout_l0': 0.3969894444456965, 'dropout_l1': 0.6781294985001394, 'lr': 0.003333803948604297, 'n_layers': 2, 'n_units_l0': 309, 'n_units_l1': 396, 'wd': 5.3821600242572735e-08}. Best is trial 15 with value: 0.838355183091938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:36:29,663] Trial 16 finished with value: 0.910355464515523 and parameters: {'dropout_l0': 0.2061359778081993, 'dropout_l1': 0.3251636211736749, 'dropout_l2': 0.49869859021811574, 'dropout_l3': 0.5410157234913415, 'dropout_l4': 0.7109254153791933, 'lr': 3.054860594431332e-05, 'n_layers': 5, 'n_units_l0': 144, 'n_units_l1': 488, 'n_units_l2': 366, 'n_units_l3': 31, 'n_units_l4': 189, 'wd': 6.28179348441096e-05}. Best is trial 15 with value: 0.838355183091938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:39:51,780] Trial 17 finished with value: 0.8247813062790113 and parameters: {'dropout_l0': 0.4208666206987251, 'dropout_l1': 0.3993485639572907, 'dropout_l2': 0.41224870897515875, 'dropout_l3': 0.6668249730198335, 'dropout_l4': 0.2171905292064047, 'lr': 0.0003374599085938831, 'n_layers': 5, 'n_units_l0': 391, 'n_units_l1': 395, 'n_units_l2': 304, 'n_units_l3': 299, 'n_units_l4': 386, 'wd': 6.282054866155932e-07}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:40:58,965] Trial 18 finished with value: 0.8977588725905133 and parameters: {'dropout_l0': 0.3936026978274104, 'lr': 5.8379505565051274e-05, 'n_layers': 1, 'n_units_l0': 428, 'wd': 1.814606508789963e-08}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:42:43,193] Trial 19 finished with value: 0.8622517662170606 and parameters: {'dropout_l0': 0.7365297141782967, 'dropout_l1': 0.5112598169421745, 'dropout_l2': 0.6914310403790256, 'lr': 0.0012966695370288275, 'n_layers': 3, 'n_units_l0': 53, 'n_units_l1': 297, 'n_units_l2': 409, 'wd': 4.4065557788224117e-05}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:45:56,950] Trial 20 finished with value: 0.8560483430185889 and parameters: {'dropout_l0': 0.5021405471432763, 'dropout_l1': 0.7009459223294241, 'dropout_l2': 0.31063164389021636, 'dropout_l3': 0.6076862395245919, 'dropout_l4': 0.5327666965344637, 'lr': 0.00020017042224432013, 'n_layers': 5, 'n_units_l0': 422, 'n_units_l1': 478, 'n_units_l2': 70, 'n_units_l3': 481, 'n_units_l4': 347, 'wd': 4.302011066792081e-08}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:47:20,872] Trial 21 finished with value: 0.9253776088739053 and parameters: {'dropout_l0': 0.3052552548101851, 'dropout_l1': 0.4132531008139939, 'lr': 2.2701534916314556e-05, 'n_layers': 2, 'n_units_l0': 166, 'n_units_l1': 225, 'wd': 1.0636633896651173e-08}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:48:22,883] Trial 22 finished with value: 0.9019629109619011 and parameters: {'dropout_l0': 0.42187768357727173, 'lr': 0.00010673578270573876, 'n_layers': 1, 'n_units_l0': 157, 'wd': 1.9448076191147951e-07}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:49:46,274] Trial 23 finished with value: 0.9154920904045432 and parameters: {'dropout_l0': 0.42265895358200933, 'dropout_l1': 0.5458622007571164, 'dropout_l2': 0.48498843135928366, 'lr': 7.030763912388559e-05, 'n_layers': 3, 'n_units_l0': 49, 'n_units_l1': 254, 'n_units_l2': 137, 'wd': 0.00048450534664539775}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 11:55:27,662] Trial 24 finished with value: 0.8262044395137037 and parameters: {'dropout_l0': 0.49230820391412006, 'dropout_l1': 0.28237056328378074, 'dropout_l2': 0.2133686761048431, 'dropout_l3': 0.6196327993354421, 'lr': 0.001062512447122155, 'n_layers': 4, 'n_units_l0': 270, 'n_units_l1': 394, 'n_units_l2': 270, 'n_units_l3': 206, 'wd': 7.22292067621667e-05}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:00:34,846] Trial 25 finished with value: 0.8294510148529314 and parameters: {'dropout_l0': 0.34626528192902933, 'dropout_l1': 0.3067952471967383, 'dropout_l2': 0.42985270809343173, 'dropout_l3': 0.5136334859442634, 'lr': 0.00029134389086559067, 'n_layers': 4, 'n_units_l0': 100, 'n_units_l1': 461, 'n_units_l2': 291, 'n_units_l3': 352, 'wd': 0.0040525877251809325}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:01:42,535] Trial 26 finished with value: 1.005168650394831 and parameters: {'dropout_l0': 0.7882297763110784, 'lr': 1.52019615167472e-05, 'n_layers': 1, 'n_units_l0': 118, 'wd': 3.097494431406309e-07}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:05:28,563] Trial 27 finished with value: 0.9540584077182998 and parameters: {'dropout_l0': 0.6265682504969061, 'dropout_l1': 0.2112635997538152, 'dropout_l2': 0.6800839508460373, 'dropout_l3': 0.24080125846850814, 'dropout_l4': 0.6818139602589535, 'lr': 2.0641059952999424e-05, 'n_layers': 5, 'n_units_l0': 456, 'n_units_l1': 235, 'n_units_l2': 377, 'n_units_l3': 17, 'n_units_l4': 402, 'wd': 0.00019466968335150466}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:09:04,976] Trial 28 finished with value: 0.8367206133328952 and parameters: {'dropout_l0': 0.47933954190540945, 'dropout_l1': 0.27103830513714633, 'dropout_l2': 0.36180272622017695, 'dropout_l3': 0.4265702819474773, 'dropout_l4': 0.47696448638332106, 'lr': 0.0008027418276041673, 'n_layers': 5, 'n_units_l0': 335, 'n_units_l1': 379, 'n_units_l2': 161, 'n_units_l3': 437, 'n_units_l4': 21, 'wd': 0.00022990583551719035}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:11:00,382] Trial 29 finished with value: 0.9288617291002192 and parameters: {'dropout_l0': 0.3399940831814007, 'dropout_l1': 0.5966437892359104, 'lr': 2.947411748461643e-05, 'n_layers': 2, 'n_units_l0': 349, 'n_units_l1': 412, 'wd': 6.233775117200516e-05}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:15:54,557] Trial 30 finished with value: 0.8338155705704648 and parameters: {'dropout_l0': 0.547391717588856, 'dropout_l1': 0.34965583628711083, 'dropout_l2': 0.23173411445898912, 'dropout_l3': 0.7182926846594602, 'lr': 0.00043878390642576034, 'n_layers': 4, 'n_units_l0': 500, 'n_units_l1': 437, 'n_units_l2': 294, 'n_units_l3': 133, 'wd': 1.3504849162555362e-05}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:20:42,422] Trial 31 finished with value: 0.8344156879645127 and parameters: {'dropout_l0': 0.5431597583468354, 'dropout_l1': 0.34971297880042745, 'dropout_l2': 0.21114537851385057, 'dropout_l3': 0.724791404330356, 'lr': 0.00042928308195550947, 'n_layers': 4, 'n_units_l0': 253, 'n_units_l1': 440, 'n_units_l2': 287, 'n_units_l3': 137, 'wd': 1.0501304909852261e-05}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:28:15,154] Trial 32 finished with value: 0.824859093906533 and parameters: {'dropout_l0': 0.4379272619445703, 'dropout_l1': 0.3553278248601408, 'dropout_l2': 0.20541892136418988, 'dropout_l3': 0.7032725412912507, 'lr': 0.0005275392926735981, 'n_layers': 4, 'n_units_l0': 216, 'n_units_l1': 438, 'n_units_l2': 284, 'n_units_l3': 146, 'wd': 1.0863045297289509e-05}. Best is trial 17 with value: 0.8247813062790113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:32:22,829] Trial 33 finished with value: 0.8218032875631609 and parameters: {'dropout_l0': 0.4268255320595128, 'dropout_l1': 0.307664929482167, 'dropout_l2': 0.4030349702314494, 'dropout_l3': 0.6907140205079577, 'lr': 0.0006568933960624271, 'n_layers': 4, 'n_units_l0': 207, 'n_units_l1': 442, 'n_units_l2': 250, 'n_units_l3': 351, 'wd': 5.723353445392017e-06}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:36:37,879] Trial 34 finished with value: 0.8282096304445186 and parameters: {'dropout_l0': 0.43038425371898764, 'dropout_l1': 0.276245525490718, 'dropout_l2': 0.20557013684357572, 'dropout_l3': 0.6821926866776449, 'lr': 0.000639209294243824, 'n_layers': 4, 'n_units_l0': 211, 'n_units_l1': 421, 'n_units_l2': 225, 'n_units_l3': 138, 'wd': 3.7359067701840584e-06}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:39:37,661] Trial 35 finished with value: 0.8306364763496269 and parameters: {'dropout_l0': 0.4592406768652836, 'dropout_l1': 0.37686579378505614, 'dropout_l2': 0.28318426914852324, 'lr': 0.000635829619662758, 'n_layers': 3, 'n_units_l0': 193, 'n_units_l1': 337, 'n_units_l2': 325, 'wd': 6.54546066372787e-06}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:44:20,877] Trial 36 finished with value: 0.8465539858891413 and parameters: {'dropout_l0': 0.440199094736557, 'dropout_l1': 0.3421630868407695, 'dropout_l2': 0.3804798674510406, 'dropout_l3': 0.7888669940712533, 'lr': 0.00019356124793728957, 'n_layers': 4, 'n_units_l0': 277, 'n_units_l1': 497, 'n_units_l2': 212, 'n_units_l3': 276, 'wd': 1.8695604544964873e-05}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:48:19,209] Trial 37 finished with value: 0.8283074814030248 and parameters: {'dropout_l0': 0.38981159660247766, 'dropout_l1': 0.27368227918361365, 'dropout_l2': 0.2816673210239473, 'dropout_l3': 0.6609263272185111, 'lr': 0.0008306560100131878, 'n_layers': 4, 'n_units_l0': 221, 'n_units_l1': 449, 'n_units_l2': 483, 'n_units_l3': 217, 'wd': 1.4079435462505095e-06}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:52:36,318] Trial 38 finished with value: 0.8282530399469229 and parameters: {'dropout_l0': 0.4625871355634966, 'dropout_l1': 0.3680333421499814, 'dropout_l2': 0.41198745960224575, 'lr': 0.0005057776855182315, 'n_layers': 3, 'n_units_l0': 285, 'n_units_l1': 342, 'n_units_l2': 262, 'wd': 5.301916998398281e-06}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 12:57:49,073] Trial 39 finished with value: 0.8339742345687671 and parameters: {'dropout_l0': 0.5205981554145604, 'dropout_l1': 0.39763098860626106, 'dropout_l2': 0.3208939650430597, 'dropout_l3': 0.6334748639102865, 'lr': 0.0002626470896608023, 'n_layers': 4, 'n_units_l0': 182, 'n_units_l1': 401, 'n_units_l2': 329, 'n_units_l3': 383, 'wd': 8.941694409522458e-07}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 13:00:43,920] Trial 40 finished with value: 0.8327862625448113 and parameters: {'dropout_l0': 0.45390954522971233, 'dropout_l1': 0.3123067924339096, 'dropout_l2': 0.2685903435575793, 'dropout_l3': 0.7065669616568933, 'lr': 0.0003391915373246738, 'n_layers': 4, 'n_units_l0': 363, 'n_units_l1': 467, 'n_units_l2': 189, 'n_units_l3': 89, 'wd': 6.834472109098823e-06}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 13:03:30,542] Trial 41 finished with value: 0.8292801920165364 and parameters: {'dropout_l0': 0.43737009602574023, 'dropout_l1': 0.28390092292691593, 'dropout_l2': 0.20852259838571338, 'dropout_l3': 0.6522783011731881, 'lr': 0.0006383257619801058, 'n_layers': 4, 'n_units_l0': 222, 'n_units_l1': 428, 'n_units_l2': 243, 'n_units_l3': 152, 'wd': 2.725841234225179e-06}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 13:05:52,592] Trial 42 finished with value: 0.8271161007066058 and parameters: {'dropout_l0': 0.4119548530103444, 'dropout_l1': 0.26426911826750804, 'dropout_l2': 0.20827350596795036, 'dropout_l3': 0.6830868624230865, 'lr': 0.000893596996453959, 'n_layers': 4, 'n_units_l0': 206, 'n_units_l1': 414, 'n_units_l2': 260, 'n_units_l3': 179, 'wd': 3.7593883671308634e-06}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 13:08:59,770] Trial 43 finished with value: 0.8264681136506236 and parameters: {'dropout_l0': 0.38072646584542796, 'dropout_l1': 0.20306897804154947, 'dropout_l2': 0.25478971159631003, 'lr': 0.0009523842411192084, 'n_layers': 3, 'n_units_l0': 238, 'n_units_l1': 357, 'n_units_l2': 326, 'wd': 2.6906665103806377e-05}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 13:11:52,133] Trial 44 finished with value: 0.8309851808425708 and parameters: {'dropout_l0': 0.3752130254436814, 'dropout_l1': 0.2063817309722953, 'dropout_l2': 0.24939913475237938, 'lr': 0.0017305054276983386, 'n_layers': 3, 'n_units_l0': 286, 'n_units_l1': 368, 'n_units_l2': 327, 'wd': 2.6182986191853242e-05}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 13:13:57,690] Trial 45 finished with value: 0.8255374635386671 and parameters: {'dropout_l0': 0.48641570388151845, 'dropout_l1': 0.3165311792875195, 'dropout_l2': 0.3160923343094135, 'lr': 0.001112868298069117, 'n_layers': 3, 'n_units_l0': 247, 'n_units_l1': 309, 'n_units_l2': 344, 'wd': 1.171741986976956e-05}. Best is trial 33 with value: 0.8218032875631609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 13:17:14,876] Trial 46 finished with value: 0.8143988083570431 and parameters: {'dropout_l0': 0.48303759321338957, 'dropout_l1': 0.32620849779889893, 'dropout_l2': 0.3163743611025014, 'dropout_l3': 0.7661053540089837, 'dropout_l4': 0.3339235798444601, 'lr': 0.0013378392548823155, 'n_layers': 5, 'n_units_l0': 265, 'n_units_l1': 306, 'n_units_l2': 286, 'n_units_l3': 309, 'n_units_l4': 499, 'wd': 1.033893175510939e-05}. Best is trial 46 with value: 0.8143988083570431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 13:20:30,909] Trial 47 finished with value: 0.8341914532531021 and parameters: {'dropout_l0': 0.4863491997111701, 'dropout_l1': 0.3254351590335851, 'dropout_l2': 0.33380553151457076, 'dropout_l3': 0.7900171062129026, 'dropout_l4': 0.30789552475744014, 'lr': 0.0022837142029598387, 'n_layers': 5, 'n_units_l0': 182, 'n_units_l1': 297, 'n_units_l2': 348, 'n_units_l3': 308, 'n_units_l4': 486, 'wd': 1.2374668930636876e-05}. Best is trial 46 with value: 0.8143988083570431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 13:24:27,805] Trial 48 finished with value: 0.8224469754430983 and parameters: {'dropout_l0': 0.5185881669826758, 'dropout_l1': 0.379288171386141, 'dropout_l2': 0.37945307612819956, 'dropout_l3': 0.7451561917838475, 'dropout_l4': 0.33514154131149, 'lr': 0.0013295357153706596, 'n_layers': 5, 'n_units_l0': 234, 'n_units_l1': 317, 'n_units_l2': 424, 'n_units_l3': 412, 'n_units_l4': 498, 'wd': 1.614930720524267e-06}. Best is trial 46 with value: 0.8143988083570431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([2185, 7]) torch.Size([2185])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n",
      "labels.shape (3122,)\n",
      "labels [1 1 1 ... 3 3 1]\n",
      "size of input and output torch.Size([468, 7]) torch.Size([468])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 13:28:06,731] Trial 49 finished with value: 0.8230133754575354 and parameters: {'dropout_l0': 0.45408947192661087, 'dropout_l1': 0.445925030929656, 'dropout_l2': 0.3850392150664324, 'dropout_l3': 0.7512143503587156, 'dropout_l4': 0.32972675387535, 'lr': 0.0014124145659893035, 'n_layers': 5, 'n_units_l0': 382, 'n_units_l1': 95, 'n_units_l2': 486, 'n_units_l3': 410, 'n_units_l4': 494, 'wd': 1.374402553065125e-06}. Best is trial 46 with value: 0.8143988083570431.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "# define the optuna study and optimize it\n",
    "objective = Objective(input_size, output_size, max_layers, max_neurons_layers, \n",
    "                      device, epochs, seed, batch_size)\n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials=n_startup_trials)\n",
    "study = optuna.create_study(study_name=study_name, sampler=sampler, storage=storage,\n",
    "                            load_if_exists=True)\n",
    "study.optimize(objective, n_trials, n_jobs=n_jobs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c6d2c-665a-4564-ac1c-af3abd136523",
   "metadata": {},
   "source": [
    "Trial 49 finished with value: 0.7992990006748427 and parameters: {'n_layers': 4, 'n_units_l0': 326, 'dropout_l0': 0.6140597590944773, 'n_units_l1': 203, 'dropout_l1': 0.28132639142015986, 'n_units_l2': 460, 'dropout_l2': 0.25855030410354674, 'n_units_l3': 337, 'dropout_l3': 0.24732580986058228, 'lr': 0.0033140422147764532, 'wd': 0.0020272408464552176}. Best is trial 49 with value: 0.7992990006748427."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8808e961-4e16-4c87-86b9-4a27c92a6e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
